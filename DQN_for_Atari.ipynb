{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN for Atari",
      "provenance": [],
      "authorship_tag": "ABX9TyO60283x/fMOKGBQQMhiBUx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/DQN_for_Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmeRd2DO4wxw"
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1TVNwJYBOBF"
      },
      "source": [
        "EPOCHS = 50\n",
        "THRESHHOLD = 10\n",
        "MONITOR = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-dkGIVeBtec"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, env_string, batch_size = 128, img_size = 84, m = 4):\n",
        "        self.memory = deque(maxlen=1000)\n",
        "        self.env = gym.make(env_string)\n",
        "        self.img_size = img_size\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.m = m\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = 1.0\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        alpha = 0.01\n",
        "        alpha_decay = 0.01\n",
        "\n",
        "        if MONITOR:\n",
        "            self.env = gym.wrappers.Monitor(self.env, 'recording', force=True)\n",
        "\n",
        "        self.model = Sequential([\n",
        "            Conv2D(32, 8, (4, 4), activation='relu', padding='same', input_shape = (img_size, img_size, m)),\n",
        "            Conv2D(64, 4, (2, 2), activation='relu', padding='valid'),\n",
        "            Conv2D(64, 3, (1, 1), activation='relu', padding='valid'),\n",
        "            Flatten(),\n",
        "            Dense(512, activation='elu'),\n",
        "            Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(alpha, decay=alpha_decay))\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def preprocces_state(self, img):\n",
        "        img_temp = img[31:195]\n",
        "        img_temp = tf.image.rgb_to_grayscale(img_temp)\n",
        "        img_temp = tf.image.resize(img_temp, (self.img_size, self.img_size),\n",
        "                                   method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        img_temp = tf.cast(img_temp, tf.float32)\n",
        "        return img_temp[:, :, 0]\n",
        "    \n",
        "    def combine_frames(self, prev, new):\n",
        "        if len(prev.shape) == 3 and prev.shape[0] == self.m:\n",
        "            imgs = np.append(prev[1:, :, :], np.expand_dims(img2, 0), axis=2)\n",
        "            return tf.expand_dims(imgs, 0)\n",
        "        else:\n",
        "            img = np.stack([prev]*self.m, axis=2)\n",
        "            return tf.expand_dims(img, 0)\n",
        "\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        x_batch, y_batch = [], []\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            y_target = self.model.predict(state)\n",
        "            if done:\n",
        "                y_target[0][action] = reward \n",
        "            else:\n",
        "                y_target[0][action] = reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
        "            x_batch.append(state[0])\n",
        "            y_batch.append(y_target[0])\n",
        "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
        "    \n",
        "    def choose_action(self, state, epsilon):\n",
        "        if np.random.random() <= epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.model.predict(state))\n",
        "    \n",
        "    def train(self):\n",
        "        scores = deque(maxlen=5)\n",
        "        avg_scores = []\n",
        "        for e in range(EPOCHS):\n",
        "            state = self.env.reset()\n",
        "            state = self.preprocces_state(state)\n",
        "            state = self.combine_frames(state, state)\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                action = self.choose_action(state, self.epsilon)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = self.preprocces_state(next_state)\n",
        "                next_state = self.combine_frames(next_state, state)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon)\n",
        "                i += reward\n",
        "            scores.append(i)\n",
        "            mean_score = np.mean(scores)\n",
        "            avg_scores.append(mean_score)\n",
        "            if mean_score >= THRESHHOLD and e >= 5:\n",
        "                print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e-5))\n",
        "                return avg_scores\n",
        "            if e % 5 == 0:\n",
        "                print('[Episode {}] - Mean survival time over last 5 episodes was {}.'.format(e, mean_score))\n",
        "            self.replay(self.batch_size)\n",
        "\n",
        "        print('Did not solve after {} episodes'.format(e))\n",
        "        return avg_scores\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgwD_usDR3pm",
        "outputId": "516b68f7-ad53-4f1f-b1c3-486520979d2d"
      },
      "source": [
        "dqn = DQN('Breakout-v0')\n",
        "dqn.train()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 5 episodes was 1.0.\n",
            "[Episode 5] - Mean survival time over last 5 episodes was 1.0.\n",
            "[Episode 10] - Mean survival time over last 5 episodes was 0.6.\n",
            "[Episode 15] - Mean survival time over last 5 episodes was 2.4.\n",
            "[Episode 20] - Mean survival time over last 5 episodes was 2.6.\n",
            "[Episode 25] - Mean survival time over last 5 episodes was 1.8.\n",
            "[Episode 30] - Mean survival time over last 5 episodes was 1.6.\n",
            "[Episode 35] - Mean survival time over last 5 episodes was 0.6.\n",
            "[Episode 40] - Mean survival time over last 5 episodes was 0.8.\n",
            "[Episode 45] - Mean survival time over last 5 episodes was 0.4.\n",
            "Did not solve after 49 episodes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0,\n",
              " 1.5,\n",
              " 1.3333333333333333,\n",
              " 1.0,\n",
              " 1.2,\n",
              " 1.0,\n",
              " 0.6,\n",
              " 0.8,\n",
              " 0.8,\n",
              " 0.6,\n",
              " 0.6,\n",
              " 1.2,\n",
              " 1.6,\n",
              " 2.0,\n",
              " 2.4,\n",
              " 2.4,\n",
              " 2.6,\n",
              " 2.6,\n",
              " 2.2,\n",
              " 2.0,\n",
              " 2.6,\n",
              " 2.8,\n",
              " 2.0,\n",
              " 2.4,\n",
              " 2.4,\n",
              " 1.8,\n",
              " 1.2,\n",
              " 1.4,\n",
              " 1.2,\n",
              " 1.2,\n",
              " 1.6,\n",
              " 1.2,\n",
              " 1.0,\n",
              " 0.8,\n",
              " 0.4,\n",
              " 0.6,\n",
              " 0.8,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.2,\n",
              " 0.8,\n",
              " 0.8,\n",
              " 0.6,\n",
              " 0.6,\n",
              " 0.6,\n",
              " 0.4,\n",
              " 0.6,\n",
              " 0.8,\n",
              " 1.0,\n",
              " 0.8]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}