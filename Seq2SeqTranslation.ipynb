{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqTranslation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSAZhtfW943mS+eCEx8ae6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/Seq2SeqTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yf9_GcB2I4b"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "\r\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\r\n",
        "if not os.path.exists('./datasets'):\r\n",
        "  os.mkdir('./datasets')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBVWZsxI46j"
      },
      "source": [
        "The book, Deep Learning with tensorflow 2 and keras by PACKT, did not have an up to date version. This is my own way to extract the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWIPZbF4CPTM",
        "outputId": "3620b70e-bb9c-437c-f918-803391c49580"
      },
      "source": [
        "!wget  -P ./datasets https://www.manythings.org/anki/fra-eng.zip\r\n",
        "!unzip ./datasets/fra-eng.zip -d ./datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-10 16:39:05--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3036::ac43:adc6, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6281268 (6.0M) [application/zip]\n",
            "Saving to: ‘./datasets/fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   5.99M  9.22MB/s    in 0.6s    \n",
            "\n",
            "2021-02-10 16:39:06 (9.22 MB/s) - ‘./datasets/fra-eng.zip’ saved [6281268/6281268]\n",
            "\n",
            "Archive:  ./datasets/fra-eng.zip\n",
            "  inflating: ./datasets/_about.txt   \n",
            "  inflating: ./datasets/fra.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uS7DwDb_XdM"
      },
      "source": [
        "def preprocess_sentence(sent):\r\n",
        "  sent = ''.join([c for c in unicodedata.normalize('NFD', sent)])\r\n",
        "  sent = re.sub(r'([!.?])', r' \\1', sent)\r\n",
        "  sent = re.sub(r'[^a-zA-Z!.?]+', r' ', sent)\r\n",
        "  sent = re.sub(r'\\s+', r' ', sent)\r\n",
        "  sent = sent.lower()\r\n",
        "  return sent"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-QXLZ1-AOdb"
      },
      "source": [
        "def download_and_read(num_pairs=50000):\r\n",
        "    en_sents, fr_sents_in, fr_sents_out = [], [], []\r\n",
        "    local_file = os.path.join(\"datasets\", \"fra.txt\")\r\n",
        "    with open(local_file, \"r\") as fin:\r\n",
        "        for i, line in enumerate(fin):\r\n",
        "            en_sent, fr_sent = line.strip().split('CC-BY')[0].strip().split('\\t')\r\n",
        "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\r\n",
        "            fr_sent = preprocess_sentence(fr_sent)\r\n",
        "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\r\n",
        "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\r\n",
        "            en_sents.append(en_sent)\r\n",
        "            fr_sents_in.append(fr_sent_in)\r\n",
        "            fr_sents_out.append(fr_sent_out)\r\n",
        "            if i >= num_pairs-1:\r\n",
        "              break\r\n",
        "    return en_sents, fr_sents_in, fr_sents_out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIR2c4_VEm47",
        "outputId": "7dbe5c9e-0f18-4394-8efe-699eb7099340"
      },
      "source": [
        "sents_en, sents_fr_in, sents_fr_out = download_and_read()\r\n",
        "\r\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters='',lower=False)\r\n",
        "tokenizer_en.fit_on_texts(sents_en)\r\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en)\r\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_en, padding='post')\r\n",
        "\r\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters=\"\", lower=False)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_in)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_out)\r\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\r\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\r\n",
        "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\r\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\r\n",
        "\r\n",
        "vocab_size_en = len(tokenizer_en.word_index)\r\n",
        "vocab_size_fr = len(tokenizer_fr.word_index)\r\n",
        "\r\n",
        "word2idx_en = tokenizer_en.word_index\r\n",
        "idx2word_en = {v:k for k,v in word2idx_en.items()}\r\n",
        "\r\n",
        "word2idx_fr = tokenizer_fr.word_index\r\n",
        "idx2word_fr = {v:k for k,v in word2idx_fr.items()}\r\n",
        "\r\n",
        "print('english vocabulary: ', str (vocab_size_en))\r\n",
        "print('french vocabulary: ', str (vocab_size_fr))\r\n",
        "\r\n",
        "maxlen_en = data_en.shape[1]\r\n",
        "maxlen_fr = data_fr_out.shape[1]\r\n",
        "print('The maximum english length is: {:d} '.format(maxlen_en) +   \r\n",
        "  'and the maximum french length is: {:d}'.format(maxlen_fr))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english vocabulary:  5889\n",
            "french vocabulary:  9017\n",
            "The maximum english length is: 8 and the maximum french length is: 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aATMp3fvQTWA"
      },
      "source": [
        "batch_size = 64\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (data_en, data_fr_in, data_fr_out))\r\n",
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = 5000 \r\n",
        "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\r\n",
        "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcjmnnRLU8Ve"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps, \r\n",
        "            encoder_dim, **kwargs):\r\n",
        "        super(Encoder, self).__init__(**kwargs)\r\n",
        "        self.encoder_dim = encoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            encoder_dim, return_sequences=False, return_state=True)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, initial_state=state)\r\n",
        "        return x, state\r\n",
        "\r\n",
        "    def init_state(self, batch_size):\r\n",
        "        return tf.zeros((batch_size, self.encoder_dim))\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps,\r\n",
        "            decoder_dim, **kwargs):\r\n",
        "        super(Decoder, self).__init__(**kwargs)\r\n",
        "        self.decoder_dim = decoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            decoder_dim, return_sequences=True, return_state=True)\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, state)\r\n",
        "        x = self.dense(x)\r\n",
        "        return x, state\r\n",
        "\r\n",
        "\r\n",
        "embedding_dim = 128\r\n",
        "encoder_dim, decoder_dim = 512, 512"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dBWPrSF9QLK"
      },
      "source": [
        "def build_graph():\r\n",
        "  encoder = Encoder(vocab_size_en+1,embedding_dim, maxlen_en, encoder_dim)\r\n",
        "  decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)\r\n",
        "  return encoder, decoder"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BxikKzbJKZE"
      },
      "source": [
        "  def loss_fn(ytrue, ypred):\r\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\r\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\r\n",
        "    loss = scce(ytrue, ypred, sample_weight=mask)\r\n",
        "    return loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvVLflXFJLEq"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(encoder, decoder, encoder_in, decoder_in, decoder_out, encoder_state):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "    decoder_state = encoder_state\r\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "    loss = loss_fn(decoder_out, decoder_pred)\r\n",
        "    \r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "  gradients = tape.gradient(loss, variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "  return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ijqXw6ZEme"
      },
      "source": [
        "def predict(encoder, decoder, sents_en, data_en,\r\n",
        "            sents_fr_out, word2idx_fr, idx2word_fr):\r\n",
        "  random_id = np.random.choice(len(sents_en))\r\n",
        "  print('input   : ', ' '.join(sents_en[random_id]))\r\n",
        "  print('label   : ', ' '.join(sents_fr_out[random_id]))\r\n",
        "  encoder_in = tf.expand_dims(data_en[random_id],axis=0)\r\n",
        "  decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\r\n",
        "  encoder_state = encoder.init_state(1)\r\n",
        "  encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "  decoder_state = encoder_state\r\n",
        "  decoder_in = tf.expand_dims(tf.constant([word2idx_fr['BOS']]), axis=0)\r\n",
        "  pred_sent_fr = []\r\n",
        "  while True:\r\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1)\r\n",
        "    pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\r\n",
        "    pred_sent_fr.append(pred_word)\r\n",
        "    if pred_word == 'EOS':\r\n",
        "      break\r\n",
        "    decoder_in = decoder_pred\r\n",
        "  \r\n",
        "  print('predicted   : ', ' '.join(pred_sent_fr[0:-1]))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3JHx7ZBdFSH"
      },
      "source": [
        "def evaluate_bleu_score(encoder, decoder, test_dataset, \r\n",
        "        word2idx_fr, idx2word_fr):\r\n",
        "\r\n",
        "    bleu_scores = []\r\n",
        "    smooth_fn = SmoothingFunction()\r\n",
        "    for encoder_in, decoder_in, decoder_out in test_dataset:\r\n",
        "        encoder_state = encoder.init_state(batch_size)\r\n",
        "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "        decoder_state = encoder_state\r\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "\r\n",
        "        # compute argmax\r\n",
        "        decoder_out = decoder_out.numpy()\r\n",
        "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\r\n",
        "\r\n",
        "        for i in range(decoder_out.shape[0]):\r\n",
        "            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\r\n",
        "            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\r\n",
        "            # remove trailing EOS\r\n",
        "            ref_sent = ref_sent[0:-1]\r\n",
        "            hyp_sent = hyp_sent[0:-1]\r\n",
        "            bleu_score = sentence_bleu([ref_sent], hyp_sent, \r\n",
        "                smoothing_function=smooth_fn.method1)\r\n",
        "            bleu_scores.append(bleu_score)\r\n",
        "\r\n",
        "    return np.mean(np.array(bleu_scores))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kque7khna1Ic"
      },
      "source": [
        "This will take up to an hour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsY1W3mzehRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355c1e2f-9428-454a-b5c0-d9c30399fcce"
      },
      "source": [
        "encoder, decoder = build_graph()\r\n",
        "if not os.path.exists('./checkpoints'):\r\n",
        "  os.mkdir('./checkpoints')\r\n",
        "num_epochs = 200\r\n",
        "for e in range(1,num_epochs+1):\r\n",
        "  encoder_state = encoder.init_state(batch_size)\r\n",
        "  for batch, data in enumerate(train_dataset):\r\n",
        "    encoder_in, decoder_in, decoder_out = data \r\n",
        "    loss = train_step(encoder, decoder, encoder_in, decoder_in, decoder_out, encoder_state)\r\n",
        "  eval_score = evaluate_bleu_score(encoder, decoder,test_dataset,word2idx_fr, idx2word_fr)\r\n",
        "  print('Epoch: {}/{}, Loss: {:.4f}, Eval Score: {:.3e}'.format(e,num_epochs, loss.numpy(),eval_score))\r\n",
        "  if e % 50 == 0:\r\n",
        "    print('SAVING CHECKPOINT {} ...'.format(e // 50))\r\n",
        "    encoder.save_weights('encoder_ckpt_{}.h5'.format(str (e // 50)))\r\n",
        "    decoder.save_weights('decoder_ckpt_{}.h5'.format(str (e // 50)))\r\n",
        "    print('COMPLETED SAVE, PREDICTING TEXT \\n')\r\n",
        "    predict(encoder, decoder, sents_en, data_en, sents_fr_out,word2idx_fr, idx2word_fr)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/200, Loss: 1.4331, Eval Score: 1.927e-02\n",
            "Epoch: 2/200, Loss: 0.8677, Eval Score: 2.852e-02\n",
            "Epoch: 3/200, Loss: 0.7526, Eval Score: 3.501e-02\n",
            "Epoch: 4/200, Loss: 0.6137, Eval Score: 4.214e-02\n",
            "Epoch: 5/200, Loss: 0.4833, Eval Score: 5.248e-02\n",
            "Epoch: 6/200, Loss: 0.3719, Eval Score: 6.297e-02\n",
            "Epoch: 7/200, Loss: 0.3488, Eval Score: 7.494e-02\n",
            "Epoch: 8/200, Loss: 0.2760, Eval Score: 8.433e-02\n",
            "Epoch: 9/200, Loss: 0.2142, Eval Score: 9.337e-02\n",
            "Epoch: 10/200, Loss: 0.2087, Eval Score: 1.031e-01\n",
            "Epoch: 11/200, Loss: 0.1705, Eval Score: 1.110e-01\n",
            "Epoch: 12/200, Loss: 0.1683, Eval Score: 1.186e-01\n",
            "Epoch: 13/200, Loss: 0.1550, Eval Score: 1.216e-01\n",
            "Epoch: 14/200, Loss: 0.1591, Eval Score: 1.255e-01\n",
            "Epoch: 15/200, Loss: 0.1526, Eval Score: 1.276e-01\n",
            "Epoch: 16/200, Loss: 0.1511, Eval Score: 1.330e-01\n",
            "Epoch: 17/200, Loss: 0.1455, Eval Score: 1.368e-01\n",
            "Epoch: 18/200, Loss: 0.1145, Eval Score: 1.382e-01\n",
            "Epoch: 19/200, Loss: 0.1113, Eval Score: 1.387e-01\n",
            "Epoch: 20/200, Loss: 0.1210, Eval Score: 1.386e-01\n",
            "Epoch: 21/200, Loss: 0.1331, Eval Score: 1.428e-01\n",
            "Epoch: 22/200, Loss: 0.0887, Eval Score: 1.430e-01\n",
            "Epoch: 23/200, Loss: 0.1157, Eval Score: 1.445e-01\n",
            "Epoch: 24/200, Loss: 0.1131, Eval Score: 1.429e-01\n",
            "Epoch: 25/200, Loss: 0.1229, Eval Score: 1.456e-01\n",
            "Epoch: 26/200, Loss: 0.1179, Eval Score: 1.481e-01\n",
            "Epoch: 27/200, Loss: 0.1103, Eval Score: 1.487e-01\n",
            "Epoch: 28/200, Loss: 0.1073, Eval Score: 1.484e-01\n",
            "Epoch: 29/200, Loss: 0.1327, Eval Score: 1.467e-01\n",
            "Epoch: 30/200, Loss: 0.1126, Eval Score: 1.504e-01\n",
            "Epoch: 31/200, Loss: 0.1176, Eval Score: 1.510e-01\n",
            "Epoch: 32/200, Loss: 0.1392, Eval Score: 1.491e-01\n",
            "Epoch: 33/200, Loss: 0.1335, Eval Score: 1.534e-01\n",
            "Epoch: 34/200, Loss: 0.1247, Eval Score: 1.513e-01\n",
            "Epoch: 35/200, Loss: 0.1060, Eval Score: 1.506e-01\n",
            "Epoch: 36/200, Loss: 0.0996, Eval Score: 1.509e-01\n",
            "Epoch: 37/200, Loss: 0.0905, Eval Score: 1.497e-01\n",
            "Epoch: 38/200, Loss: 0.0777, Eval Score: 1.526e-01\n",
            "Epoch: 39/200, Loss: 0.1205, Eval Score: 1.521e-01\n",
            "Epoch: 40/200, Loss: 0.1100, Eval Score: 1.513e-01\n",
            "Epoch: 41/200, Loss: 0.0822, Eval Score: 1.523e-01\n",
            "Epoch: 42/200, Loss: 0.0998, Eval Score: 1.522e-01\n",
            "Epoch: 43/200, Loss: 0.0857, Eval Score: 1.534e-01\n",
            "Epoch: 44/200, Loss: 0.1115, Eval Score: 1.557e-01\n",
            "Epoch: 45/200, Loss: 0.0985, Eval Score: 1.521e-01\n",
            "Epoch: 46/200, Loss: 0.1290, Eval Score: 1.534e-01\n",
            "Epoch: 47/200, Loss: 0.0956, Eval Score: 1.519e-01\n",
            "Epoch: 48/200, Loss: 0.1093, Eval Score: 1.538e-01\n",
            "Epoch: 49/200, Loss: 0.1147, Eval Score: 1.522e-01\n",
            "Epoch: 50/200, Loss: 0.0702, Eval Score: 1.552e-01\n",
            "SAVING CHECKPOINT 1 ...\n",
            "COMPLETED SAVE, PREDICTING TEXT \n",
            "\n",
            "input   :  sorry about the mess .\n",
            "label   :  de sole pour la pagaille ! EOS\n",
            "predicted   :  de sole e pour la pagaille !\n",
            "Epoch: 51/200, Loss: 0.1085, Eval Score: 1.524e-01\n",
            "Epoch: 52/200, Loss: 0.1151, Eval Score: 1.541e-01\n",
            "Epoch: 53/200, Loss: 0.1077, Eval Score: 1.540e-01\n",
            "Epoch: 54/200, Loss: 0.1199, Eval Score: 1.544e-01\n",
            "Epoch: 55/200, Loss: 0.1083, Eval Score: 1.539e-01\n",
            "Epoch: 56/200, Loss: 0.0799, Eval Score: 1.561e-01\n",
            "Epoch: 57/200, Loss: 0.0909, Eval Score: 1.535e-01\n",
            "Epoch: 58/200, Loss: 0.0968, Eval Score: 1.530e-01\n",
            "Epoch: 59/200, Loss: 0.1128, Eval Score: 1.549e-01\n",
            "Epoch: 60/200, Loss: 0.1011, Eval Score: 1.541e-01\n",
            "Epoch: 61/200, Loss: 0.1050, Eval Score: 1.558e-01\n",
            "Epoch: 62/200, Loss: 0.0906, Eval Score: 1.550e-01\n",
            "Epoch: 63/200, Loss: 0.0968, Eval Score: 1.545e-01\n",
            "Epoch: 64/200, Loss: 0.0757, Eval Score: 1.546e-01\n",
            "Epoch: 65/200, Loss: 0.0882, Eval Score: 1.564e-01\n",
            "Epoch: 66/200, Loss: 0.0856, Eval Score: 1.540e-01\n",
            "Epoch: 67/200, Loss: 0.1168, Eval Score: 1.543e-01\n",
            "Epoch: 68/200, Loss: 0.1139, Eval Score: 1.548e-01\n",
            "Epoch: 69/200, Loss: 0.1086, Eval Score: 1.544e-01\n",
            "Epoch: 70/200, Loss: 0.0855, Eval Score: 1.537e-01\n",
            "Epoch: 71/200, Loss: 0.1074, Eval Score: 1.539e-01\n",
            "Epoch: 72/200, Loss: 0.0932, Eval Score: 1.551e-01\n",
            "Epoch: 73/200, Loss: 0.0956, Eval Score: 1.537e-01\n",
            "Epoch: 74/200, Loss: 0.0775, Eval Score: 1.550e-01\n",
            "Epoch: 75/200, Loss: 0.1177, Eval Score: 1.568e-01\n",
            "Epoch: 76/200, Loss: 0.1066, Eval Score: 1.518e-01\n",
            "Epoch: 77/200, Loss: 0.0957, Eval Score: 1.571e-01\n",
            "Epoch: 78/200, Loss: 0.1055, Eval Score: 1.534e-01\n",
            "Epoch: 79/200, Loss: 0.1183, Eval Score: 1.559e-01\n",
            "Epoch: 80/200, Loss: 0.0868, Eval Score: 1.589e-01\n",
            "Epoch: 81/200, Loss: 0.1121, Eval Score: 1.538e-01\n",
            "Epoch: 82/200, Loss: 0.0971, Eval Score: 1.532e-01\n",
            "Epoch: 83/200, Loss: 0.1000, Eval Score: 1.552e-01\n",
            "Epoch: 84/200, Loss: 0.0649, Eval Score: 1.543e-01\n",
            "Epoch: 85/200, Loss: 0.0737, Eval Score: 1.558e-01\n",
            "Epoch: 86/200, Loss: 0.0914, Eval Score: 1.555e-01\n",
            "Epoch: 87/200, Loss: 0.0990, Eval Score: 1.554e-01\n",
            "Epoch: 88/200, Loss: 0.0852, Eval Score: 1.568e-01\n",
            "Epoch: 89/200, Loss: 0.0777, Eval Score: 1.559e-01\n",
            "Epoch: 90/200, Loss: 0.0861, Eval Score: 1.558e-01\n",
            "Epoch: 91/200, Loss: 0.1008, Eval Score: 1.553e-01\n",
            "Epoch: 92/200, Loss: 0.1048, Eval Score: 1.560e-01\n",
            "Epoch: 93/200, Loss: 0.0888, Eval Score: 1.571e-01\n",
            "Epoch: 94/200, Loss: 0.1067, Eval Score: 1.537e-01\n",
            "Epoch: 95/200, Loss: 0.1093, Eval Score: 1.557e-01\n",
            "Epoch: 96/200, Loss: 0.0905, Eval Score: 1.591e-01\n",
            "Epoch: 97/200, Loss: 0.0808, Eval Score: 1.546e-01\n",
            "Epoch: 98/200, Loss: 0.0795, Eval Score: 1.559e-01\n",
            "Epoch: 99/200, Loss: 0.0718, Eval Score: 1.575e-01\n",
            "Epoch: 100/200, Loss: 0.0953, Eval Score: 1.574e-01\n",
            "SAVING CHECKPOINT 2 ...\n",
            "COMPLETED SAVE, PREDICTING TEXT \n",
            "\n",
            "input   :  turn everything off .\n",
            "label   :  e teignez tout . EOS\n",
            "predicted   :  e teignez tout .\n",
            "Epoch: 101/200, Loss: 0.0923, Eval Score: 1.559e-01\n",
            "Epoch: 102/200, Loss: 0.0773, Eval Score: 1.548e-01\n",
            "Epoch: 103/200, Loss: 0.0878, Eval Score: 1.575e-01\n",
            "Epoch: 104/200, Loss: 0.0778, Eval Score: 1.555e-01\n",
            "Epoch: 105/200, Loss: 0.0891, Eval Score: 1.578e-01\n",
            "Epoch: 106/200, Loss: 0.1005, Eval Score: 1.570e-01\n",
            "Epoch: 107/200, Loss: 0.0784, Eval Score: 1.568e-01\n",
            "Epoch: 108/200, Loss: 0.0754, Eval Score: 1.567e-01\n",
            "Epoch: 109/200, Loss: 0.0760, Eval Score: 1.602e-01\n",
            "Epoch: 110/200, Loss: 0.0754, Eval Score: 1.568e-01\n",
            "Epoch: 111/200, Loss: 0.0829, Eval Score: 1.570e-01\n",
            "Epoch: 112/200, Loss: 0.1033, Eval Score: 1.546e-01\n",
            "Epoch: 113/200, Loss: 0.0872, Eval Score: 1.556e-01\n",
            "Epoch: 114/200, Loss: 0.0792, Eval Score: 1.566e-01\n",
            "Epoch: 115/200, Loss: 0.1033, Eval Score: 1.578e-01\n",
            "Epoch: 116/200, Loss: 0.0758, Eval Score: 1.573e-01\n",
            "Epoch: 117/200, Loss: 0.0711, Eval Score: 1.570e-01\n",
            "Epoch: 118/200, Loss: 0.0822, Eval Score: 1.561e-01\n",
            "Epoch: 119/200, Loss: 0.0935, Eval Score: 1.548e-01\n",
            "Epoch: 120/200, Loss: 0.0734, Eval Score: 1.532e-01\n",
            "Epoch: 121/200, Loss: 0.0904, Eval Score: 1.571e-01\n",
            "Epoch: 122/200, Loss: 0.0954, Eval Score: 1.563e-01\n",
            "Epoch: 123/200, Loss: 0.0757, Eval Score: 1.573e-01\n",
            "Epoch: 124/200, Loss: 0.0752, Eval Score: 1.563e-01\n",
            "Epoch: 125/200, Loss: 0.0728, Eval Score: 1.563e-01\n",
            "Epoch: 126/200, Loss: 0.0856, Eval Score: 1.563e-01\n",
            "Epoch: 127/200, Loss: 0.0915, Eval Score: 1.575e-01\n",
            "Epoch: 128/200, Loss: 0.0803, Eval Score: 1.554e-01\n",
            "Epoch: 129/200, Loss: 0.1050, Eval Score: 1.580e-01\n",
            "Epoch: 130/200, Loss: 0.0672, Eval Score: 1.558e-01\n",
            "Epoch: 131/200, Loss: 0.0811, Eval Score: 1.561e-01\n",
            "Epoch: 132/200, Loss: 0.0650, Eval Score: 1.587e-01\n",
            "Epoch: 133/200, Loss: 0.0887, Eval Score: 1.598e-01\n",
            "Epoch: 134/200, Loss: 0.0787, Eval Score: 1.547e-01\n",
            "Epoch: 135/200, Loss: 0.0938, Eval Score: 1.570e-01\n",
            "Epoch: 136/200, Loss: 0.0880, Eval Score: 1.543e-01\n",
            "Epoch: 137/200, Loss: 0.0997, Eval Score: 1.580e-01\n",
            "Epoch: 138/200, Loss: 0.0749, Eval Score: 1.551e-01\n",
            "Epoch: 139/200, Loss: 0.0762, Eval Score: 1.565e-01\n",
            "Epoch: 140/200, Loss: 0.0654, Eval Score: 1.584e-01\n",
            "Epoch: 141/200, Loss: 0.1073, Eval Score: 1.578e-01\n",
            "Epoch: 142/200, Loss: 0.0932, Eval Score: 1.578e-01\n",
            "Epoch: 143/200, Loss: 0.0768, Eval Score: 1.569e-01\n",
            "Epoch: 144/200, Loss: 0.0599, Eval Score: 1.574e-01\n",
            "Epoch: 145/200, Loss: 0.0574, Eval Score: 1.561e-01\n",
            "Epoch: 146/200, Loss: 0.0846, Eval Score: 1.590e-01\n",
            "Epoch: 147/200, Loss: 0.0743, Eval Score: 1.581e-01\n",
            "Epoch: 148/200, Loss: 0.0841, Eval Score: 1.583e-01\n",
            "Epoch: 149/200, Loss: 0.0796, Eval Score: 1.577e-01\n",
            "Epoch: 150/200, Loss: 0.0836, Eval Score: 1.539e-01\n",
            "SAVING CHECKPOINT 3 ...\n",
            "COMPLETED SAVE, PREDICTING TEXT \n",
            "\n",
            "input   :  i can t go .\n",
            "label   :  je ne peux pas m y rendre . EOS\n",
            "predicted   :  je ne peux pas m y rendre .\n",
            "Epoch: 151/200, Loss: 0.0842, Eval Score: 1.580e-01\n",
            "Epoch: 152/200, Loss: 0.0747, Eval Score: 1.580e-01\n",
            "Epoch: 153/200, Loss: 0.0691, Eval Score: 1.571e-01\n",
            "Epoch: 154/200, Loss: 0.0778, Eval Score: 1.563e-01\n",
            "Epoch: 155/200, Loss: 0.0732, Eval Score: 1.596e-01\n",
            "Epoch: 156/200, Loss: 0.0860, Eval Score: 1.553e-01\n",
            "Epoch: 157/200, Loss: 0.0845, Eval Score: 1.576e-01\n",
            "Epoch: 158/200, Loss: 0.0635, Eval Score: 1.570e-01\n",
            "Epoch: 159/200, Loss: 0.0797, Eval Score: 1.538e-01\n",
            "Epoch: 160/200, Loss: 0.1096, Eval Score: 1.581e-01\n",
            "Epoch: 161/200, Loss: 0.0836, Eval Score: 1.562e-01\n",
            "Epoch: 162/200, Loss: 0.0827, Eval Score: 1.592e-01\n",
            "Epoch: 163/200, Loss: 0.0701, Eval Score: 1.579e-01\n",
            "Epoch: 164/200, Loss: 0.0732, Eval Score: 1.579e-01\n",
            "Epoch: 165/200, Loss: 0.0684, Eval Score: 1.588e-01\n",
            "Epoch: 166/200, Loss: 0.0864, Eval Score: 1.589e-01\n",
            "Epoch: 167/200, Loss: 0.0813, Eval Score: 1.543e-01\n",
            "Epoch: 168/200, Loss: 0.0840, Eval Score: 1.572e-01\n",
            "Epoch: 169/200, Loss: 0.0894, Eval Score: 1.589e-01\n",
            "Epoch: 170/200, Loss: 0.0684, Eval Score: 1.574e-01\n",
            "Epoch: 171/200, Loss: 0.0874, Eval Score: 1.579e-01\n",
            "Epoch: 172/200, Loss: 0.0766, Eval Score: 1.558e-01\n",
            "Epoch: 173/200, Loss: 0.0660, Eval Score: 1.556e-01\n",
            "Epoch: 174/200, Loss: 0.0714, Eval Score: 1.582e-01\n",
            "Epoch: 175/200, Loss: 0.0675, Eval Score: 1.572e-01\n",
            "Epoch: 176/200, Loss: 0.0700, Eval Score: 1.566e-01\n",
            "Epoch: 177/200, Loss: 0.0783, Eval Score: 1.582e-01\n",
            "Epoch: 178/200, Loss: 0.0948, Eval Score: 1.563e-01\n",
            "Epoch: 179/200, Loss: 0.0910, Eval Score: 1.580e-01\n",
            "Epoch: 180/200, Loss: 0.0630, Eval Score: 1.563e-01\n",
            "Epoch: 181/200, Loss: 0.0591, Eval Score: 1.562e-01\n",
            "Epoch: 182/200, Loss: 0.0804, Eval Score: 1.553e-01\n",
            "Epoch: 183/200, Loss: 0.0751, Eval Score: 1.586e-01\n",
            "Epoch: 184/200, Loss: 0.0845, Eval Score: 1.569e-01\n",
            "Epoch: 185/200, Loss: 0.0896, Eval Score: 1.574e-01\n",
            "Epoch: 186/200, Loss: 0.0714, Eval Score: 1.586e-01\n",
            "Epoch: 187/200, Loss: 0.0879, Eval Score: 1.559e-01\n",
            "Epoch: 188/200, Loss: 0.0737, Eval Score: 1.578e-01\n",
            "Epoch: 189/200, Loss: 0.0690, Eval Score: 1.551e-01\n",
            "Epoch: 190/200, Loss: 0.0752, Eval Score: 1.568e-01\n",
            "Epoch: 191/200, Loss: 0.0822, Eval Score: 1.577e-01\n",
            "Epoch: 192/200, Loss: 0.0791, Eval Score: 1.557e-01\n",
            "Epoch: 193/200, Loss: 0.0786, Eval Score: 1.574e-01\n",
            "Epoch: 194/200, Loss: 0.0821, Eval Score: 1.562e-01\n",
            "Epoch: 195/200, Loss: 0.0716, Eval Score: 1.579e-01\n",
            "Epoch: 196/200, Loss: 0.0686, Eval Score: 1.574e-01\n",
            "Epoch: 197/200, Loss: 0.0686, Eval Score: 1.552e-01\n",
            "Epoch: 198/200, Loss: 0.0795, Eval Score: 1.571e-01\n",
            "Epoch: 199/200, Loss: 0.0840, Eval Score: 1.586e-01\n",
            "Epoch: 200/200, Loss: 0.1116, Eval Score: 1.573e-01\n",
            "SAVING CHECKPOINT 4 ...\n",
            "COMPLETED SAVE, PREDICTING TEXT \n",
            "\n",
            "input   :  tom acted alone .\n",
            "label   :  tom a agi seul . EOS\n",
            "predicted   :  tom a agi seul .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}