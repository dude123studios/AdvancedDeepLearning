{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla AutoEncoder",
      "provenance": [],
      "authorship_tag": "ABX9TyMXM6GBJwjJO6QCF7HRRG3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/Vanilla_AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvBGkLsMHVb2"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras as K\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwSKsO9AHoui"
      },
      "source": [
        "np.random.seed(11)\r\n",
        "tf.random.set_seed(11)\r\n",
        "batch_size = 256\r\n",
        "epochs = 50\r\n",
        "learning_rate = 1e-3\r\n",
        "momentum = 8e-1\r\n",
        "hidden_dim = 128\r\n",
        "original_dim = 784"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UjTTfw3Ikii"
      },
      "source": [
        "(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\r\n",
        "x_train = x_train / 255.\r\n",
        "x_test = x_test / 255.\r\n",
        "x_train = x_train.astype(np.float32)\r\n",
        "x_test = x_test.astype(np.float32)\r\n",
        "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1]*x_train.shape[2]))\r\n",
        "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\r\n",
        "train_data = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xro22u41p2OV"
      },
      "source": [
        "class Encoder(K.layers.Layer):\r\n",
        "  def __init__(self, num_units):\r\n",
        "    super(Encoder, self).__init__(num_units)\r\n",
        "    self.dense = K.layers.Dense(num_units, activation='relu')\r\n",
        "  def call(self, x):\r\n",
        "    x = self.dense(x)\r\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxDraXnrv6tJ"
      },
      "source": [
        "class Decoder(K.layers.Layer):\r\n",
        "  def __init__(self, num_units):\r\n",
        "    super(Decoder, self).__init__(num_units)\r\n",
        "    self.dense = K.layers.Dense(num_units, activation='relu')\r\n",
        "  def call(self, x):\r\n",
        "    x = self.dense(x)\r\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mfRZqOpwrf-"
      },
      "source": [
        "class Autoencoder(K.Model):\r\n",
        "  def __init__(self, hidden_dim, original_dim):\r\n",
        "    super(Autoencoder, self).__init__()\r\n",
        "    self.loss = []\r\n",
        "    self.encoder = Encoder(hidden_dim)\r\n",
        "    self.decoder = Decoder(original_dim)\r\n",
        "\r\n",
        "  def call(self, input_features):\r\n",
        "    encoded = self.encoder(input_features)\r\n",
        "    reconstructed = self.decoder(encoded)\r\n",
        "    return reconstructed"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8bltlP_3To"
      },
      "source": [
        "autoencoder = Autoencoder(hidden_dim, original_dim)\r\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-2)\r\n",
        "def loss(preds, real):\r\n",
        "  return tf.reduce_mean(tf.square(tf.subtract(real, preds)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZcUP9GRAhZl"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(loss, model, opt, original):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    preds = model(original)\r\n",
        "    error = loss(preds, original)\r\n",
        "    gradients = tape.gradient(error, model.trainable_variables)\r\n",
        "    vars = zip(gradients, model.trainable_variables)\r\n",
        "  opt.apply_gradients(vars)\r\n",
        "  return error"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYyFrkaoBHpv"
      },
      "source": [
        "def train(model, opt, loss, dataset, epochs=20):\r\n",
        "  for epoch in range(1, epochs):\r\n",
        "    epoch_loss = 0\r\n",
        "    for step, batch_features in enumerate(dataset):\r\n",
        "      loss_val = train_step(loss, model, opt, batch_features)\r\n",
        "      epoch_loss += loss_val\r\n",
        "    model.loss.append(epoch_loss)\r\n",
        "    print('Epoch {}/{}. Loss: {}'.format(epoch, epochs, epoch_loss.numpy()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvmC_3ZjCSTy",
        "outputId": "5c1e7ea0-9f88-4ee0-b077-8400368be4bd"
      },
      "source": [
        "train(autoencoder, opt, loss, train_data, epochs = epochs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50. Loss: 5.159158229827881\n",
            "Epoch 2/50. Loss: 3.0579447746276855\n",
            "Epoch 3/50. Loss: 2.952500343322754\n",
            "Epoch 4/50. Loss: 2.920259952545166\n",
            "Epoch 5/50. Loss: 2.8625288009643555\n",
            "Epoch 6/50. Loss: 2.8708505630493164\n",
            "Epoch 7/50. Loss: 2.840562343597412\n",
            "Epoch 8/50. Loss: 2.8197474479675293\n",
            "Epoch 9/50. Loss: 2.835493803024292\n",
            "Epoch 10/50. Loss: 2.805487632751465\n",
            "Epoch 11/50. Loss: 2.836609363555908\n",
            "Epoch 12/50. Loss: 2.817033052444458\n",
            "Epoch 13/50. Loss: 2.8194468021392822\n",
            "Epoch 14/50. Loss: 2.8027093410491943\n",
            "Epoch 15/50. Loss: 2.7878048419952393\n",
            "Epoch 16/50. Loss: 2.8020923137664795\n",
            "Epoch 17/50. Loss: 2.8114423751831055\n",
            "Epoch 18/50. Loss: 2.8193395137786865\n",
            "Epoch 19/50. Loss: 2.797562837600708\n",
            "Epoch 20/50. Loss: 2.786923408508301\n",
            "Epoch 21/50. Loss: 2.8186817169189453\n",
            "Epoch 22/50. Loss: 2.789909601211548\n",
            "Epoch 23/50. Loss: 2.8077011108398438\n",
            "Epoch 24/50. Loss: 2.7880916595458984\n",
            "Epoch 25/50. Loss: 2.7963504791259766\n",
            "Epoch 26/50. Loss: 2.8264358043670654\n",
            "Epoch 27/50. Loss: 2.7998740673065186\n",
            "Epoch 28/50. Loss: 2.7762601375579834\n",
            "Epoch 29/50. Loss: 2.806994915008545\n",
            "Epoch 30/50. Loss: 2.7935023307800293\n",
            "Epoch 31/50. Loss: 2.798900604248047\n",
            "Epoch 32/50. Loss: 2.7991275787353516\n",
            "Epoch 33/50. Loss: 2.812182664871216\n",
            "Epoch 34/50. Loss: 2.801227331161499\n",
            "Epoch 35/50. Loss: 2.801572322845459\n",
            "Epoch 36/50. Loss: 2.828249931335449\n",
            "Epoch 37/50. Loss: 2.7885324954986572\n",
            "Epoch 38/50. Loss: 2.788339376449585\n",
            "Epoch 39/50. Loss: 2.8151090145111084\n",
            "Epoch 40/50. Loss: 2.8082807064056396\n",
            "Epoch 41/50. Loss: 2.7936227321624756\n",
            "Epoch 42/50. Loss: 2.8290553092956543\n",
            "Epoch 43/50. Loss: 2.8049116134643555\n",
            "Epoch 44/50. Loss: 2.775843381881714\n",
            "Epoch 45/50. Loss: 2.8118646144866943\n",
            "Epoch 46/50. Loss: 2.7927467823028564\n",
            "Epoch 47/50. Loss: 2.8041765689849854\n",
            "Epoch 48/50. Loss: 2.8135974407196045\n",
            "Epoch 49/50. Loss: 2.8221733570098877\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}