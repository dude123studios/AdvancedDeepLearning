{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqTranslation With Attention",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkIFxQXVyjfepxvqds8ghh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/Seq2SeqTranslation_With_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yf9_GcB2I4b"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\r\n",
        "if not os.path.exists('./datasets'):\r\n",
        "  os.mkdir('./datasets')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBVWZsxI46j"
      },
      "source": [
        "The book, Deep Learning with tensorflow 2 and keras by PACKT, did not have an up to date version. This is my own way to extract the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWIPZbF4CPTM",
        "outputId": "0d4ae2d1-7561-45c8-8f18-7abbffe9223b"
      },
      "source": [
        "!wget  -P ./datasets https://www.manythings.org/anki/spa-eng.zip\r\n",
        "!unzip ./datasets/spa-eng.zip -d ./datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-13 18:23:17--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.21.55.222, 2606:4700:3036::ac43:adc6, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4934182 (4.7M) [application/zip]\n",
            "Saving to: ‘./datasets/spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.71M  15.6MB/s    in 0.3s    \n",
            "\n",
            "2021-02-13 18:23:17 (15.6 MB/s) - ‘./datasets/spa-eng.zip’ saved [4934182/4934182]\n",
            "\n",
            "Archive:  ./datasets/spa-eng.zip\n",
            "  inflating: ./datasets/_about.txt   \n",
            "  inflating: ./datasets/spa.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uS7DwDb_XdM"
      },
      "source": [
        "def preprocess_sentence(sent):\r\n",
        "  sent = ''.join([c for c in unicodedata.normalize('NFD', sent)])\r\n",
        "  sent = re.sub(r'([!.?])', r' \\1', sent)\r\n",
        "  sent = re.sub(r'[^a-zA-Z!.?]+', r' ', sent)\r\n",
        "  sent = re.sub(r'\\s+', r' ', sent)\r\n",
        "  sent = sent.lower()\r\n",
        "  return sent"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-QXLZ1-AOdb"
      },
      "source": [
        "def download_and_read(num_pairs=50000):\r\n",
        "    en_sents, sp_sents_in, sp_sents_out = [], [], []\r\n",
        "    local_file = os.path.join(\"datasets\", \"spa.txt\")\r\n",
        "    with open(local_file, \"r\") as fin:\r\n",
        "        for i, line in enumerate(fin):\r\n",
        "            en_sent, sp_sent = line.strip().split('CC-BY')[0].strip().split('\\t')\r\n",
        "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\r\n",
        "            sp_sent = preprocess_sentence(sp_sent)\r\n",
        "            sp_sent_in = [w for w in (\"BOS \" + sp_sent).split()]\r\n",
        "            sp_sent_out = [w for w in (sp_sent + \" EOS\").split()]\r\n",
        "            en_sents.append(en_sent)\r\n",
        "            sp_sents_in.append(sp_sent_in)\r\n",
        "            sp_sents_out.append(sp_sent_out)\r\n",
        "            if i >= num_pairs-1:\r\n",
        "              break\r\n",
        "    return en_sents, sp_sents_in, sp_sents_out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIR2c4_VEm47",
        "outputId": "7c329b6d-108e-4808-9192-f9d4ba9f8a8c"
      },
      "source": [
        "sents_en, sents_sp_in, sents_sp_out = download_and_read()\r\n",
        "\r\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters='',lower=False)\r\n",
        "tokenizer_en.fit_on_texts(sents_en)\r\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en)\r\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_en, padding='post')\r\n",
        "\r\n",
        "tokenizer_sp = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters=\"\", lower=False)\r\n",
        "tokenizer_sp.fit_on_texts(sents_sp_in)\r\n",
        "tokenizer_sp.fit_on_texts(sents_sp_out)\r\n",
        "data_sp_in = tokenizer_sp.texts_to_sequences(sents_sp_in)\r\n",
        "data_sp_in = tf.keras.preprocessing.sequence.pad_sequences(data_sp_in, padding=\"post\")\r\n",
        "data_sp_out = tokenizer_sp.texts_to_sequences(sents_sp_out)\r\n",
        "data_sp_out = tf.keras.preprocessing.sequence.pad_sequences(data_sp_out, padding=\"post\")\r\n",
        "\r\n",
        "vocab_size_en = len(tokenizer_en.word_index)\r\n",
        "vocab_size_sp = len(tokenizer_sp.word_index)\r\n",
        "\r\n",
        "word2idx_en = tokenizer_en.word_index\r\n",
        "idx2word_en = {v:k for k,v in word2idx_en.items()}\r\n",
        "\r\n",
        "word2idx_sp = tokenizer_sp.word_index\r\n",
        "idx2word_sp = {v:k for k,v in word2idx_sp.items()}\r\n",
        "\r\n",
        "print('english vocabulary: ', str (vocab_size_en))\r\n",
        "print('spanish vocabulary: ', str (vocab_size_sp))\r\n",
        "\r\n",
        "maxlen_en = data_en.shape[1]\r\n",
        "maxlen_sp = data_sp_out.shape[1]\r\n",
        "print('The maximum english length is: {:d} '.format(maxlen_en) +   \r\n",
        "  'and the maximum spanish length is: {:d}'.format(maxlen_sp))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english vocabulary:  6639\n",
            "spanish vocabulary:  12364\n",
            "The maximum english length is: 10 and the maximum spanish length is: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aATMp3fvQTWA"
      },
      "source": [
        "batch_size = 64\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (data_en, data_sp_in, data_sp_out))\r\n",
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = 5000 \r\n",
        "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\r\n",
        "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkg99Q2DHFG5"
      },
      "source": [
        "class BahdanauAttention(Layer):\r\n",
        "  def __init__(self, num_units):\r\n",
        "    super(BahdanauAttention, self).__init__()\r\n",
        "    self.W1 = Dense(num_units)\r\n",
        "    self.W2 = Dense(num_units)\r\n",
        "    self.V = Dense(1)\r\n",
        "  \r\n",
        "  def call(self, query, values):\r\n",
        "    \r\n",
        "    query = tf.expand_dims(query, axis=1)\r\n",
        "    distribution = self.V(tf.nn.tanh(self.W1(query)+ self.W2(values)))\r\n",
        "    distribution = tf.nn.softmax(distribution)\r\n",
        "\r\n",
        "    context = tf.reduce_sum(\r\n",
        "        tf.linalg.matmul(\r\n",
        "            tf.linalg.matrix_transpose(distribution),\r\n",
        "            values\r\n",
        "        ),axis=1\r\n",
        "    )\r\n",
        "    #context = tf.expand_dims(context,axis=1)\r\n",
        "    return context, distribution"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcjmnnRLU8Ve"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps, \r\n",
        "            encoder_dim, **kwargs):\r\n",
        "        super(Encoder, self).__init__(**kwargs)\r\n",
        "        self.encoder_dim = encoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            encoder_dim, return_sequences=True, return_state=True)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, initial_state=state)\r\n",
        "        return x, state\r\n",
        "\r\n",
        "    def init_state(self, batch_size):\r\n",
        "        return tf.zeros((batch_size, self.encoder_dim))\r\n",
        "\r\n",
        "class Decoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, decoder_dim, **kwargs):\r\n",
        "    super(Decoder, self).__init__(**kwargs)\r\n",
        "    self.decoder_dim = decoder_dim\r\n",
        "    self.attention = BahdanauAttention(embedding_dim)\r\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\r\n",
        "    self.rnn = GRU(decoder_dim, return_sequences=True, return_state=True)\r\n",
        "    self.Ws = Dense(vocab_size)\r\n",
        "\r\n",
        "  def call(self, x, state, encoder_out):\r\n",
        "    x = self.embedding(x)\r\n",
        "    context, alignment = self.attention(state, encoder_out)\r\n",
        "    x = tf.expand_dims(tf.concat([context, x], axis=-1),axis=1)\r\n",
        "    x, state = self.rnn(x)\r\n",
        "    x = self.Ws(x)\r\n",
        "    return x, state, alignment\r\n",
        "\r\n",
        "embedding_dim = 256\r\n",
        "encoder_dim, decoder_dim = 256, 256"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dBWPrSF9QLK"
      },
      "source": [
        "def build_graph():\r\n",
        "  encoder = Encoder(vocab_size_en+1,embedding_dim, maxlen_en, encoder_dim)\r\n",
        "  decoder = Decoder(vocab_size_sp+1, embedding_dim, decoder_dim)\r\n",
        "  return encoder, decoder"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BxikKzbJKZE"
      },
      "source": [
        "  def loss_fn(ytrue, ypred):\r\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\r\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\r\n",
        "    loss = scce(ytrue, ypred, sample_weight=mask)\r\n",
        "    return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvVLflXFJLEq"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(encoder, decoder, encoder_in, decoder_in, decoder_out, encoder_state):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "    decoder_state = encoder_state\r\n",
        "    loss = 0\r\n",
        "    for t in range(decoder_out.shape[1]):\r\n",
        "      decoder_in_t = decoder_in[:, t]\r\n",
        "      decoder_pred_t, decoder_state, _ = decoder(decoder_in_t, decoder_state, encoder_out)\r\n",
        "      loss += loss_fn(decoder_out[:, t], decoder_pred_t)\r\n",
        "\r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "  gradients = tape.gradient(loss, variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "  return loss / decoder_out.shape[1]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ijqXw6ZEme"
      },
      "source": [
        "def predict(encoder, decoder, sents_en, data_en,\r\n",
        "            sents_sp_out, word2idx_sp, idx2word_sp):\r\n",
        "  random_id = np.random.choice(len(sents_en))\r\n",
        "  print('input   : ', ' '.join(sents_en[random_id]))\r\n",
        "  print('label   : ', ' '.join(sents_sp_out[random_id][0:-1]))\r\n",
        "  encoder_in = tf.expand_dims(data_en[random_id],axis=0)\r\n",
        "  decoder_out = tf.expand_dims(sents_sp_out[random_id], axis=0)\r\n",
        "  encoder_state = encoder.init_state(1)\r\n",
        "  encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "  decoder_state = encoder_state\r\n",
        "  decoder_in = tf.constant([word2idx_sp['BOS']])\r\n",
        "  pred_sent_sp = []\r\n",
        "  while True:\r\n",
        "    decoder_pred, decoder_state, _ = decoder(decoder_in, decoder_state, encoder_out)\r\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1)\r\n",
        "    pred_word = idx2word_sp[decoder_pred.numpy()[0][0]]\r\n",
        "    pred_sent_sp.append(pred_word)\r\n",
        "    if pred_word == 'EOS':\r\n",
        "      break\r\n",
        "    decoder_in = decoder_pred[0]\r\n",
        "  print('predicted   : ', ' '.join(pred_sent_sp))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsY1W3mzehRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f61172b-c827-4675-f1f2-c3a1c5f5bdc6"
      },
      "source": [
        " with tf.device('gpu:0'):\r\n",
        "  encoder, decoder = build_graph()\r\n",
        "  if not os.path.exists('./checkpoints'):\r\n",
        "    os.mkdir('./checkpoints')\r\n",
        "  num_epochs = 50\r\n",
        "  for e in range(1,num_epochs+1):\r\n",
        "    encoder_state = encoder.init_state(batch_size)\r\n",
        "    for batch, data in enumerate(train_dataset):\r\n",
        "      encoder_in, decoder_in, decoder_out = data \r\n",
        "      loss = train_step(encoder, decoder, encoder_in, decoder_in, decoder_out, encoder_state)\r\n",
        "    print('Epoch: {}/{}, Loss: {:.4f}'.format(e,num_epochs, loss.numpy()))\r\n",
        "    if e % 5 == 0:\r\n",
        "      print('PREDICTING TEXT \\n')\r\n",
        "      predict(encoder, decoder, sents_en, data_en, sents_sp_out,word2idx_sp, idx2word_sp)\r\n",
        "      print('\\n')\r\n",
        "      if e % 2 == 0:\r\n",
        "        print('SAVING CHECKPOINT {} ...'.format(str (e // 10)))\r\n",
        "        encoder.save_weights('./checkpoints/encoder_ckpt_{}.h5'.format(str (e // 10)))\r\n",
        "        decoder.save_weights('./checkpoints/decoder_ckpt_{}.h5'.format(str (e // 10)))\r\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50, Loss: 2.1898\n",
            "Epoch: 2/50, Loss: 2.1438\n",
            "Epoch: 3/50, Loss: 1.2886\n",
            "Epoch: 4/50, Loss: 0.9837\n",
            "Epoch: 5/50, Loss: 0.8524\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  i don t quite follow you .\n",
            "label   :  no veo a do nde vas .\n",
            "predicted   :  no te he dicho . EOS\n",
            "\n",
            "\n",
            "Epoch: 6/50, Loss: 0.5914\n",
            "Epoch: 7/50, Loss: 0.4836\n",
            "Epoch: 8/50, Loss: 0.3837\n",
            "Epoch: 9/50, Loss: 0.3739\n",
            "Epoch: 10/50, Loss: 0.3307\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  be kind .\n",
            "label   :  sean gentiles .\n",
            "predicted   :  se dico . EOS\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 1 ...\n",
            "Epoch: 11/50, Loss: 0.3014\n",
            "Epoch: 12/50, Loss: 0.2539\n",
            "Epoch: 13/50, Loss: 0.2039\n",
            "Epoch: 14/50, Loss: 0.1743\n",
            "Epoch: 15/50, Loss: 0.1419\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  i will help you all i can .\n",
            "label   :  te ayudare en todo lo que pueda .\n",
            "predicted   :  te ayudare en todo lo que pueda . EOS\n",
            "\n",
            "\n",
            "Epoch: 16/50, Loss: 0.1659\n",
            "Epoch: 17/50, Loss: 0.1370\n",
            "Epoch: 18/50, Loss: 0.1019\n",
            "Epoch: 19/50, Loss: 0.1091\n",
            "Epoch: 20/50, Loss: 0.0971\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  this is a triangle .\n",
            "label   :  esto es un tria ngulo .\n",
            "predicted   :  esto es un tria ngulo . EOS\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 2 ...\n",
            "Epoch: 21/50, Loss: 0.1122\n",
            "Epoch: 22/50, Loss: 0.0962\n",
            "Epoch: 23/50, Loss: 0.1092\n",
            "Epoch: 24/50, Loss: 0.1029\n",
            "Epoch: 25/50, Loss: 0.0824\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  we succeeded !\n",
            "label   :  lo conseguimos !\n",
            "predicted   :  lo conseguimos ! EOS\n",
            "\n",
            "\n",
            "Epoch: 26/50, Loss: 0.0802\n",
            "Epoch: 27/50, Loss: 0.0887\n",
            "Epoch: 28/50, Loss: 0.0880\n",
            "Epoch: 29/50, Loss: 0.0793\n",
            "Epoch: 30/50, Loss: 0.0751\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  i m here to protect you .\n",
            "label   :  estoy aqui para protegerte .\n",
            "predicted   :  estoy aqui para protegerte . EOS\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 3 ...\n",
            "Epoch: 31/50, Loss: 0.0702\n",
            "Epoch: 32/50, Loss: 0.0666\n",
            "Epoch: 33/50, Loss: 0.0867\n",
            "Epoch: 34/50, Loss: 0.0692\n",
            "Epoch: 35/50, Loss: 0.0709\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  he is a clever boy .\n",
            "label   :  e l es un nin o astuto .\n",
            "predicted   :  e l es un nin o astuto . EOS\n",
            "\n",
            "\n",
            "Epoch: 36/50, Loss: 0.1173\n",
            "Epoch: 37/50, Loss: 0.0861\n",
            "Epoch: 38/50, Loss: 0.0546\n",
            "Epoch: 39/50, Loss: 0.0886\n",
            "Epoch: 40/50, Loss: 0.0964\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  we re famished .\n",
            "label   :  estamos muertos de hambre .\n",
            "predicted   :  estamos muertos de hambre . EOS\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 4 ...\n",
            "Epoch: 41/50, Loss: 0.0532\n",
            "Epoch: 42/50, Loss: 0.0652\n",
            "Epoch: 43/50, Loss: 0.0800\n",
            "Epoch: 44/50, Loss: 0.0825\n",
            "Epoch: 45/50, Loss: 0.0667\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  what do you want to buy ?\n",
            "label   :  que desea comprar ?\n",
            "predicted   :  que quieres comprar ? EOS\n",
            "\n",
            "\n",
            "Epoch: 46/50, Loss: 0.0729\n",
            "Epoch: 47/50, Loss: 0.0811\n",
            "Epoch: 48/50, Loss: 0.0837\n",
            "Epoch: 49/50, Loss: 0.0782\n",
            "Epoch: 50/50, Loss: 0.0645\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  they saved us .\n",
            "label   :  ellos nos salvaron .\n",
            "predicted   :  ellos nos salvaron . EOS\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 5 ...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}