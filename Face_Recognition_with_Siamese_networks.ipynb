{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Recognition with Siamese networks",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOB/HuI6i3XCfl+Zq1xrffp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/Face_Recognition_with_Siamese_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SygHWZxrAhaf"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRhhNBSyCr1O"
      },
      "source": [
        "Data is from https://www.kaggle.com/vasukipatel/face-recognition-dataset, and includes different celeberty images of about 40~ different classes. We will train our network to check if two different images are of the same class. In this way, we will be able to generalize to any use case of people. We will use the kaggle api to download the dataset. In order to try this yourself, you would need a file called kaggle.json on your computer, which you can get from the kaggle homepage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "aPKvkNdlPMXJ",
        "outputId": "c83d4fc7-00a8-4203-b4b0-4a15d7aa1acd"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload() # Browse for the kaggle.json file that you downloaded\r\n",
        "\r\n",
        "# Make directory named kaggle, copy kaggle.json file there, and change the permissions of the file.\r\n",
        "! mkdir ~/.kaggle\r\n",
        "! cp kaggle.json ~/.kaggle/\r\n",
        "! chmod 600 ~/.kaggle/kaggle.json\r\n",
        "\r\n",
        "# You can check if everything's okay by running this command.\r\n",
        "! kaggle datasets list\r\n",
        "\r\n",
        "# Download and unzip sign-language-mnist dataset into '/usr/local'\r\n",
        "! kaggle datasets download -d vasukipatel/face-recognition-dataset --path '/usr/local' --unzip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e6907bbd-117c-4fa8-a628-d26478c0faa3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e6907bbd-117c-4fa8-a628-d26478c0faa3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "ref                                                       title                                         size  lastUpdated          downloadCount  \n",
            "--------------------------------------------------------  -------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-wallstreetsbets-posts                       Reddit WallStreetBets Posts                    7MB  2021-02-18 07:47:29           1282  \n",
            "michau96/restaurant-business-rankings-2020                Restaurant Business Rankings 2020             16KB  2021-01-30 14:20:45           2092  \n",
            "yorkehead/stock-market-subreddits                         Stock Market Subreddits                        1MB  2021-01-29 13:53:50            225  \n",
            "aagghh/crypto-telegram-groups                             Crypto telegram groups                       297MB  2021-02-02 09:58:25            149  \n",
            "timoboz/superbowl-history-1967-2020                       Superbowl History 1967 - 2020                  2KB  2020-02-03 23:41:14           4558  \n",
            "prondeau/superbowlads                                     Super Bowl Ads                                36KB  2020-02-04 18:04:07           1198  \n",
            "jessemostipak/african-american-achievements               African American Achievements                 28KB  2020-06-08 22:15:20            212  \n",
            "iainmcintosh/wsb-gme-reddit-submission-vs-stock-price     WSB / GME Reddit submission vs stock price   210KB  2021-01-30 15:31:16             60  \n",
            "docsouth-data/north-american-slave-narratives             North American Slave Narratives               42MB  2017-08-14 18:54:00            191  \n",
            "docsouth-data/the-church-in-the-southern-black-community  The Church in the Southern Black Community    28MB  2017-08-14 19:50:28            119  \n",
            "shivamb/netflix-shows                                     Netflix Movies and TV Shows                    1MB  2021-01-18 16:20:26          92616  \n",
            "gpreda/covid-world-vaccination-progress                   COVID-19 World Vaccination Progress           73KB  2021-02-18 07:45:51          13529  \n",
            "ayushggarg/all-trumps-twitter-insults-20152021            All Trump's Twitter insults (2015-2021)      581KB  2021-01-20 16:51:05           3218  \n",
            "arashnic/hr-analytics-job-change-of-data-scientists       HR Analytics: Job Change of Data Scientists  295KB  2020-12-07 00:25:10           8429  \n",
            "jsphyg/weather-dataset-rattle-package                     Rain in Australia                              4MB  2020-12-11 10:26:12          33565  \n",
            "sakshigoyal7/credit-card-customers                        Credit Card customers                        379KB  2020-11-19 07:38:44          25914  \n",
            "gregorut/videogamesales                                   Video Game Sales                             381KB  2016-10-26 09:10:49         191969  \n",
            "gpreda/pfizer-vaccine-tweets                              Pfizer Vaccine Tweets                        937KB  2021-02-18 07:45:09           4327  \n",
            "datasnaek/youtube-new                                     Trending YouTube Video Statistics            201MB  2019-06-03 00:56:47         127801  \n",
            "google/tinyquickdraw                                      QuickDraw Sketches                            11GB  2018-04-18 19:38:04           2836  \n",
            "Downloading face-recognition-dataset.zip to /usr/local\n",
            " 97% 707M/726M [00:03<00:00, 232MB/s]\n",
            "100% 726M/726M [00:03<00:00, 241MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WkylwjgLmJU"
      },
      "source": [
        "def preprocces_img(image_path):\r\n",
        "  img = tf.io.read_file('/usr/local/Faces/Faces/'+image_path)\r\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\r\n",
        "  img = tf.image.resize(img, (64,64))\r\n",
        "  #use mobilenet preproccesing for convinience, even if we use a smaller model\r\n",
        "  img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\r\n",
        "  return img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELb-UuVFChv-"
      },
      "source": [
        "df = pd.read_csv('/usr/local/Dataset.csv')\r\n",
        "face_urls = df['id'].tolist()\r\n",
        "labels = df['label'].tolist()\r\n",
        "classes = set(label for label in labels)\r\n",
        "classes = list(classes)\r\n",
        "faces = []\r\n",
        "pairs1 = []\r\n",
        "pairs2 = []\r\n",
        "for class_ in classes:\r\n",
        "  faces.append([preprocces_img(face_url) for i, face_url in enumerate(face_urls) if labels[i]==class_])\r\n",
        "\r\n",
        "#produces about 100,000 image pairs, so we only take 40,000 for training time\r\n",
        "total = 40000\r\n",
        "labels = []\r\n",
        "congruent_label = np.asarray([1])\r\n",
        "for class_ in faces:\r\n",
        "  for i in range(total // len(classes)):\r\n",
        "    j = random.randint(0,len(class_)-2)\r\n",
        "    k = random.randint(j+1, len(class_)-1)\r\n",
        "    pairs1.append(class_[j])\r\n",
        "    pairs2.append(class_[k])\r\n",
        "    labels.append(congruent_label)\r\n",
        "#all non congruent pairs form 3m pairs which is far too much, \r\n",
        "#so we will randomly choose 30,000\r\n",
        "#giving us a more reasonable 60,000 total images, since a lot of epochs will be required\r\n",
        "non_congruent_label = np.asarray([0])\r\n",
        "for i in range(total):\r\n",
        "  j = random.randint(0,len(faces)-2)\r\n",
        "  k = random.randint(j+1,len(faces)-1)\r\n",
        "  l = random.randint(0,len(faces[j])-1)\r\n",
        "  m = random.randint(0,len(faces[k])-1)\r\n",
        "  pairs1.append(faces[j][l])\r\n",
        "  pairs2.append(faces[k][m])\r\n",
        "  labels.append(non_congruent_label)\r\n",
        "del faces\r\n",
        "del face_urls\r\n",
        "del classes"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIDVfCcdAH6M"
      },
      "source": [
        "pairs1 = np.asarray(pairs1)\r\n",
        "pairs2 = np.asarray(pairs2)\r\n",
        "labels = np.asarray(labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6fqtRkWf89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7128eea5-e299-458d-9b7e-0bf96f98ef5d"
      },
      "source": [
        "'''\r\n",
        "train_size = 50000\r\n",
        "test_size = 5000\r\n",
        "val_size = 5000\r\n",
        "test_dataset = dataset.take(test_size).batch(128, drop_remainder=True)\r\n",
        "val_dataset = dataset.skip(test_size).take(val_size).batch(128, drop_remainder=True)\r\n",
        "train_dataset = dataset.skip(test_size+val_size).batch(128, drop_remainder=True)\r\n",
        "del dataset\r\n",
        "'''\r\n",
        "#we will only train on this notebook"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntrain_size = 50000\\ntest_size = 5000\\nval_size = 5000\\ntest_dataset = dataset.take(test_size).batch(128, drop_remainder=True)\\nval_dataset = dataset.skip(test_size).take(val_size).batch(128, drop_remainder=True)\\ntrain_dataset = dataset.skip(test_size+val_size).batch(128, drop_remainder=True)\\ndel dataset\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwm9i6A0vNZs"
      },
      "source": [
        "Now the fun part! lets define the model. We have one model which learns about facial features with conv layers and another that learns facial diferentiation  with just a couple of dense layers to a singular nueron which will be 1 for true and 0 for false wether or not two faces are of the same person"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PEnWbEbwo58"
      },
      "source": [
        "class CNN_Block(tf.keras.Model):\r\n",
        "  def __init__(self, num_filters, conv_size):\r\n",
        "    super(CNN_Block, self).__init__()\r\n",
        "    self.conv = Conv2D(num_filters, conv_size, padding='same')\r\n",
        "    self.bn = BatchNormalization()\r\n",
        "    self.dropout = Dropout(0.3)\r\n",
        "    self.pool = MaxPool2D()\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    x = self.conv(x)\r\n",
        "    x = self.bn(x)\r\n",
        "    x = tf.nn.relu(x)\r\n",
        "    x = self.dropout(x)\r\n",
        "    x = self.pool(x)\r\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa-dasdwCcrK"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.block1 = CNN_Block(32, 3)\r\n",
        "    self.block2 = CNN_Block(64, 3)\r\n",
        "    self.block3 = CNN_Block(128, 3)\r\n",
        "    self.block4 = CNN_Block(256, 3)\r\n",
        "    self.flatten = Flatten()\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    x = self.block1(x)\r\n",
        "    x = self.block2(x)\r\n",
        "    x = self.block3(x)\r\n",
        "    x = self.block4(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXGJx49Jy6XR"
      },
      "source": [
        "class Euclidean_Distance(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(Euclidean_Distance, self).__init__()\r\n",
        "    self.dense = Dense(1, activation='sigmoid')\r\n",
        "\r\n",
        "  def call(self, xA, xB):\r\n",
        "    sumSquared = tf.keras.backend.sum(tf.keras.backend.square(xA - xB), axis=-1, keepdims=True)\r\n",
        "    distance = tf.keras.backend.sqrt(tf.keras.backend.maximum(sumSquared, tf.keras.backend.epsilon()))\r\n",
        "    a = self.dense(distance)\r\n",
        "    return a"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqiEbzfM_oNJ"
      },
      "source": [
        "def build_model():\r\n",
        "  imgA = Input((64,64,3), dtype=tf.float32, name='inputA')\r\n",
        "  imgB = Input((64,64,3), dtype=tf.float32, name='inputB')\r\n",
        "  encoder = Encoder()\r\n",
        "  featA = encoder(imgA)\r\n",
        "  featB = encoder(imgB)\r\n",
        "  dist = Euclidean_Distance()\r\n",
        "  outputs = dist(featA, featB)\r\n",
        "  model = tf.keras.Model(inputs=(imgA, imgB),outputs=outputs)\r\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam')\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpSYGtR7VjZP",
        "outputId": "57f29181-6ad9-4911-967f-557dfd37ebf4"
      },
      "source": [
        "with tf.device('gpu:0'):\r\n",
        "  inputs = {'inputA':pairs1, 'inputB':pairs2}\r\n",
        "  model = build_model()\r\n",
        "  model.fit(inputs, labels,shuffle=True, epochs=50)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputA (InputLayer)             [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inputB (InputLayer)             [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Encoder)               (None, 4096)         390336      inputA[0][0]                     \n",
            "                                                                 inputB[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "euclidean__distance (Euclidean_ (None, 1)            2           encoder[0][0]                    \n",
            "                                                                 encoder[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 390,338\n",
            "Trainable params: 389,378\n",
            "Non-trainable params: 960\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "2500/2500 [==============================] - 54s 18ms/step - loss: 1.8159\n",
            "Epoch 2/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6950\n",
            "Epoch 3/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6933\n",
            "Epoch 4/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6941\n",
            "Epoch 5/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6928\n",
            "Epoch 6/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6923\n",
            "Epoch 7/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6898\n",
            "Epoch 8/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6885\n",
            "Epoch 9/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6854\n",
            "Epoch 10/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6824\n",
            "Epoch 11/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6790\n",
            "Epoch 12/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6756\n",
            "Epoch 13/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6733\n",
            "Epoch 14/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6709\n",
            "Epoch 15/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6674\n",
            "Epoch 16/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6641\n",
            "Epoch 17/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6616\n",
            "Epoch 18/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6584\n",
            "Epoch 19/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6570\n",
            "Epoch 20/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6553\n",
            "Epoch 21/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6553\n",
            "Epoch 22/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6512\n",
            "Epoch 23/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6496\n",
            "Epoch 24/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6518\n",
            "Epoch 25/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6495\n",
            "Epoch 26/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6488\n",
            "Epoch 27/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6458\n",
            "Epoch 28/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6429\n",
            "Epoch 29/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6431\n",
            "Epoch 30/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6415\n",
            "Epoch 31/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6420\n",
            "Epoch 32/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6400\n",
            "Epoch 33/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6372\n",
            "Epoch 34/50\n",
            "2500/2500 [==============================] - 46s 18ms/step - loss: 0.6374\n",
            "Epoch 35/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6348\n",
            "Epoch 36/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6384\n",
            "Epoch 37/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6347\n",
            "Epoch 38/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6316\n",
            "Epoch 39/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6366\n",
            "Epoch 40/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6320\n",
            "Epoch 41/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6307\n",
            "Epoch 42/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6304\n",
            "Epoch 43/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6312\n",
            "Epoch 44/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6306\n",
            "Epoch 45/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6316\n",
            "Epoch 46/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6284\n",
            "Epoch 47/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6279\n",
            "Epoch 48/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6284\n",
            "Epoch 49/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6278\n",
            "Epoch 50/50\n",
            "2500/2500 [==============================] - 45s 18ms/step - loss: 0.6255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFnOuBNTPwyO"
      },
      "source": [
        "model.save_weights('face_recogniton.h5')"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}