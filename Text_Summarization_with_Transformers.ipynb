{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summerization with Transformers",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnD7a+VSPNS0iuSpWzL031",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/Text_Summarization_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpySg_Z9sOAq"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow.keras.layers as l\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "DdZZd8G2tcnF",
        "outputId": "61c54fdc-e615-4faf-8b76-c69d8e83100f"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload() # Browse for the kaggle.json file that you downloaded\r\n",
        "\r\n",
        "# Make directory named kaggle, copy kaggle.json file there, and change the permissions of the file.\r\n",
        "! mkdir ~/.kaggle\r\n",
        "! cp kaggle.json ~/.kaggle/\r\n",
        "! chmod 600 ~/.kaggle/kaggle.json\r\n",
        "\r\n",
        "! kaggle datasets download -d pariza/bbc-news-summary --path '/usr/local' --unzip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d2f6ba3a-f006-40e0-bdc2-19fe330bd858\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d2f6ba3a-f006-40e0-bdc2-19fe330bd858\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading bbc-news-summary.zip to /usr/local\n",
            "  0% 0.00/8.91M [00:00<?, ?B/s]\n",
            "100% 8.91M/8.91M [00:00<00:00, 82.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzCP08h6vND-"
      },
      "source": [
        "def generate_articles(path):\r\n",
        "    articles = []\r\n",
        "    for sub in os.listdir(path):\r\n",
        "        article = ''\r\n",
        "        try: \r\n",
        "            f = open(os.path.join(path, sub))\r\n",
        "            for line in f:\r\n",
        "                article += line.strip()\r\n",
        "            articles.append(article)\r\n",
        "        except:\r\n",
        "            article = 'error'\r\n",
        "            articles.append(article)\r\n",
        "    return articles"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oAo8IlL2aw1"
      },
      "source": [
        "texts = []\r\n",
        "texts.extend(generate_articles('/usr/local/BBC News Summary/News Articles/business'))\r\n",
        "texts.extend(generate_articles('/usr/local/BBC News Summary/News Articles/entertainment'))\r\n",
        "texts.extend(generate_articles('/usr/local/BBC News Summary/News Articles/politics'))\r\n",
        "texts.extend(generate_articles('/usr/local/BBC News Summary/News Articles/sport'))\r\n",
        "texts.extend(generate_articles('/usr/local/BBC News Summary/News Articles/tech'))\r\n",
        "\r\n",
        "summaries = []\r\n",
        "summaries.extend(generate_articles('/usr/local/BBC News Summary/Summaries/business'))\r\n",
        "summaries.extend(generate_articles('/usr/local/BBC News Summary/Summaries/entertainment'))\r\n",
        "summaries.extend(generate_articles('/usr/local/BBC News Summary/Summaries/politics'))\r\n",
        "summaries.extend(generate_articles('/usr/local/BBC News Summary/Summaries/sport'))\r\n",
        "summaries.extend(generate_articles('/usr/local/BBC News Summary/Summaries/tech'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx5aUe7R3dH3"
      },
      "source": [
        "def tokenize(text, vocab_size = 10000, lower = True):\r\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size,\r\n",
        "                                                      oov_token='<unk>',\r\n",
        "                                                      lower=lower,\r\n",
        "                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\r\n",
        "    tokenizer.fit_on_texts(text)\r\n",
        "    tokenizer.word_index['<pad>'] = 0\r\n",
        "    tokenizer.index_word[0] = '<pad>'\r\n",
        "    word2idx = tokenizer.word_index\r\n",
        "    idx2word = {v:k for k,v in word2idx.items()}\r\n",
        "    return word2idx, idx2word, tokenizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQbFRHp65tir"
      },
      "source": [
        "texts = [f'<start> {t} <end>' for t in texts]\r\n",
        "summaries = [f'<start> {t} <end>' for t in summaries]\r\n",
        "word2idx_text, idx2word_text, text_tokenizer = tokenize(texts)\r\n",
        "word2idx_summary, idx2word_summary, summary_tokenizer = tokenize(summaries)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dBSKGoR6Fg9",
        "outputId": "aa1e50f6-2054-41af-e07f-a74c0783c8c9"
      },
      "source": [
        "sequence_lengths = np.array([len(s.split()) for s in texts])\r\n",
        "print([(p, np.percentile(sequence_lengths, p))\r\n",
        "  for p in [75, 80, 90, 95, 99, 100]\r\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 467.0), (80, 500.0), (90, 610.6000000000001), (95, 725.0), (99, 995.2799999999993), (100, 4379.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSYouiN7mfHr",
        "outputId": "1694ff38-1675-4f60-a501-c1d763822d78"
      },
      "source": [
        "sequence_lengths = np.array([len(s.split()) for s in summaries])\r\n",
        "print([(p, np.percentile(sequence_lengths, p))\r\n",
        "  for p in [75, 80, 90, 95, 99, 100]\r\n",
        "])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 204.0), (80, 221.0), (90, 270.0), (95, 315.0), (99, 434.75999999999976), (100, 2075.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z_k-d3EabY9",
        "outputId": "61e1e84f-cb0d-4342-f3c0-4a03a1ce5a50"
      },
      "source": [
        "print(texts[0])\r\n",
        "print(summaries[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> France Telecom gets Orange boostStrong growth in subscriptions to mobile phone network Orange has helped boost profits at owner France Telecom.Orange added more than five million new customers in 2004, leading to a 10% increase in its revenues. Increased take-up of broadband telecoms services also boosted France Telecom's profits, which showed a 5.5% rise to 18.3bn euros ($23.4bn; £12.5bn). France Telecom is to spend 578m euros on buying out minority shareholders in data services provider Equant.France Telecom, one of the world's largest telecoms and internet service providers, saw its full-year sales rise 2.2% to 47.2bn euros in 2004.Orange enjoyed strong growth outside France and the United Kingdom - its core markets - swelling its subscriber base to 5.4 million. France Telecom's broadband customers also increased, rising to 5.1 million across Europe by the end of the year. The firm said it had met its main strategic objectives of growing its individual businesses and further reducing its large debt. An ill-fated expansion drive in the late 1990s saw France Telecom's debt soar to 72bn euros by 2002. However, this has now been reduced to 43.9bn euros. \"Our results for 2004 allow us to improve our financial structure while focusing on the innovation that drives our strategy,\" said chief executive Thierry Breton.Looking ahead, the company forecast like-for-like sales growth of between 3% and 5% over the next three years. France Telecom is consolidating its interest in Equant, which provides telecoms and data services to businesses. Subject to approval by shareholders of the two firms, it will buy the shares in Equant it does not already own. France Telecom said it would fund the deal by selling an 8% stake in telephone directory company PagesJaunes. <end>\n",
            "<start> France Telecom, one of the world's largest telecoms and internet service providers, saw its full-year sales rise 2.2% to 47.2bn euros in 2004.Increased take-up of broadband telecoms services also boosted France Telecom's profits, which showed a 5.5% rise to 18.3bn euros ($23.4bn; £12.5bn).France Telecom is to spend 578m euros on buying out minority shareholders in data services provider Equant.France Telecom is consolidating its interest in Equant, which provides telecoms and data services to businesses.An ill-fated expansion drive in the late 1990s saw France Telecom's debt soar to 72bn euros by 2002.Strong growth in subscriptions to mobile phone network Orange has helped boost profits at owner France Telecom. <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtpMgR0wl93k"
      },
      "source": [
        "max_seqlen_text = 700\r\n",
        "text_as_ints = text_tokenizer.texts_to_sequences(texts)\r\n",
        "text_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    text_as_ints, maxlen=max_seqlen_text, padding='post')\r\n",
        "max_seqlen_summary = 701\r\n",
        "summaries_as_ints = summary_tokenizer.texts_to_sequences(summaries)\r\n",
        "summaries_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    summaries_as_ints, maxlen = max_seqlen_summary, padding='post')\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((text_as_ints, summaries_as_ints))\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0bUDdU5vUr_"
      },
      "source": [
        "    BATCH_SIZE = 64\r\n",
        "    BUFFER_SIZE = 1000\r\n",
        "    \r\n",
        "    dataset = dataset.map(lambda x, y: {\"source\": x, \"target\": y})\r\n",
        "    dataset = dataset.batch(BATCH_SIZE).shuffle(BUFFER_SIZE)\r\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuqrJQqawGqn"
      },
      "source": [
        "class TokenEmbedding(l.Layer):\r\n",
        "  def __init__(self, maxlen, num_vocab = 10000, num_hid = 64):\r\n",
        "    super().__init__()\r\n",
        "    self.embedding = l.Embedding(num_vocab + 3, num_hid)\r\n",
        "    self.pos_embedding = l.Embedding(maxlen, num_hid)\r\n",
        "  \r\n",
        "  def call(self, x):\r\n",
        "    maxlen = tf.shape(x)[-1]\r\n",
        "    x = self.embedding(x)\r\n",
        "    positions = self.pos_embedding(tf.range(0, maxlen, delta=1))\r\n",
        "    return x + positions"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt5m4ASj_d0W"
      },
      "source": [
        "class TransformerEncoder(l.Layer):\r\n",
        "  def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\r\n",
        "    super().__init__()\r\n",
        "    self.attn = l.MultiHeadAttention(num_heads, embed_dim)\r\n",
        "    self.ffn = tf.keras.Sequential([\r\n",
        "        l.Dense(feed_forward_dim, 'relu'),\r\n",
        "        l.Dense(embed_dim)                   \r\n",
        "    ])\r\n",
        "    self.norm1 = l.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.norm2 = l.LayerNormalization(epsilon=1e-6)\r\n",
        "\r\n",
        "    self.dropout1 = l.Dropout(rate=rate)\r\n",
        "    self.dropout2 = l.Dropout(rate=rate)\r\n",
        "\r\n",
        "  def call(self, inputs, training):\r\n",
        "    attn = self.attn(inputs, inputs)\r\n",
        "    attn_output = self.dropout1(attn, training=training)\r\n",
        "    out1 = self.norm1(inputs + attn)\r\n",
        "    ffn_output = self.ffn(out1)\r\n",
        "    ffn_output = self.dropout2(ffn_output)\r\n",
        "    return self.norm2(out1 + ffn_output)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkpphoB9DXTU"
      },
      "source": [
        "class TransformerDecoder(l.Layer):\r\n",
        "  def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\r\n",
        "    super().__init__()\r\n",
        "    self.norm1 = l.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.norm2 = l.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.norm3 = l.LayerNormalization(epsilon=1e-6)\r\n",
        "\r\n",
        "    self.self_att = l.MultiHeadAttention(num_heads, embed_dim)\r\n",
        "    self.enc_att = l.MultiHeadAttention(num_heads, embed_dim)\r\n",
        "\r\n",
        "    self.self_dropout = l.Dropout(rate)\r\n",
        "    self.enc_dropout = l.Dropout(rate)\r\n",
        "    self.ffn_dropout = l.Dropout(rate)\r\n",
        "    self.ffn = tf.keras.Sequential([\r\n",
        "        l.Dense(feed_forward_dim, 'relu'),\r\n",
        "        l.Dense(embed_dim)                   \r\n",
        "    ])\r\n",
        "  def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\r\n",
        "    i = tf.range(n_dest)[:, None]\r\n",
        "    j = tf.range(n_src)\r\n",
        "    m = i >= j - n_src + n_dest\r\n",
        "    mask = tf.cast(m, dtype)\r\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\r\n",
        "    mult = tf.concat(\r\n",
        "      [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\r\n",
        "    )\r\n",
        "    return tf.tile(mask, mult)\r\n",
        "  \r\n",
        "  def call(self, enc_out, target):\r\n",
        "    input_shape = tf.shape(target)\r\n",
        "    batch_size = input_shape[0]\r\n",
        "    seq_len = input_shape[1]\r\n",
        "    causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\r\n",
        "    target_attn = self.self_att(target, target, attention_mask=causal_mask)\r\n",
        "    target_norm = self.norm1(self.self_dropout(target_attn) + target)\r\n",
        "    enc_out = self.enc_att(enc_out, target_norm)\r\n",
        "    enc_out_norm = self.norm2(self.enc_dropout(enc_out) + target_norm)\r\n",
        "    ffn_out = self.ffn(enc_out_norm)\r\n",
        "    ffn_out_norm = self.norm3(self.ffn_dropout(ffn_out) + enc_out_norm)\r\n",
        "    return ffn_out_norm"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9IyDzODK7Pe"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        num_hid=128,\r\n",
        "        num_head=2,\r\n",
        "        num_feed_forward=256,\r\n",
        "        source_maxlen=700,\r\n",
        "        target_maxlen=700,\r\n",
        "        num_layers_enc=4,\r\n",
        "        num_layers_dec=1,\r\n",
        "        num_classes=10000,\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\r\n",
        "        self.num_layers_enc = num_layers_enc\r\n",
        "        self.num_layers_dec = num_layers_dec\r\n",
        "        self.target_maxlen = target_maxlen\r\n",
        "        self.num_classes = num_classes\r\n",
        "\r\n",
        "        self.enc_input = TokenEmbedding(\r\n",
        "            num_vocab=num_classes, maxlen=source_maxlen, num_hid=num_hid\r\n",
        "        )\r\n",
        "        self.dec_input = TokenEmbedding(\r\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\r\n",
        "        )\r\n",
        "\r\n",
        "        self.encoder = tf.keras.Sequential(\r\n",
        "            [self.enc_input]\r\n",
        "            + [\r\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\r\n",
        "                for _ in range(num_layers_enc)\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "        for i in range(num_layers_dec):\r\n",
        "            setattr(\r\n",
        "                self,\r\n",
        "                f\"dec_layer_{i}\",\r\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\r\n",
        "            )\r\n",
        "\r\n",
        "        self.classifier = l.Dense(num_classes)\r\n",
        "\r\n",
        "    def decode(self, enc_out, target):\r\n",
        "        y = self.dec_input(target)\r\n",
        "        for i in range(self.num_layers_dec):\r\n",
        "            y = getattr(self, f'dec_layer_{i}')(enc_out, y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        source = inputs[0]\r\n",
        "        target = inputs[1]\r\n",
        "        x = self.encoder(source)\r\n",
        "        y = self.decode(x, target)\r\n",
        "        return self.classifier(y)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def metrics(self):\r\n",
        "        return [self.loss_metric]\r\n",
        "    \r\n",
        "    def train_step(self, batch):\r\n",
        "        source = batch['source']\r\n",
        "        target = batch['target']\r\n",
        "        dec_input = target[:, :-1]\r\n",
        "        dec_target = target[:, 1:]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            preds = self([source, dec_input])\r\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\r\n",
        "            loss = self.compiled_loss(dec_target, preds, sample_weight=mask)\r\n",
        "        train_vars = self.trainable_variables\r\n",
        "        gradients = tape.gradient(loss, train_vars)\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, train_vars))\r\n",
        "        self.loss_metric.update_state(loss)\r\n",
        "        return {'loss':self.loss_metric.result()}\r\n",
        "\r\n",
        "    def test_step(self, batch):\r\n",
        "        source = batch[\"source\"]\r\n",
        "        target = batch[\"target\"]\r\n",
        "        dec_input = target[:, :-1]\r\n",
        "        dec_target = target[:, 1:]\r\n",
        "        preds = self([source, dec_input])\r\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\r\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\r\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\r\n",
        "        self.loss_metric.update_state(loss)\r\n",
        "        return {\"loss\": self.loss_metric.result()}\r\n",
        "    def generate(self, source, target_start_token_idx):\r\n",
        "        bs = tf.shape(source)[0]\r\n",
        "        enc = self.encoder(source)\r\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\r\n",
        "        dec_logits = []\r\n",
        "        for i in range(self.target_maxlen - 1):\r\n",
        "            dec_out = self.decode(enc, dec_input)\r\n",
        "            logits = self.classifier(dec_out)\r\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\r\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\r\n",
        "            dec_logits.append(last_logit)\r\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\r\n",
        "        return dec_input\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5pmLzK1zRIy"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
        "  def __init__(self, d_model, warmup_steps=4000):\r\n",
        "    super(CustomSchedule, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\r\n",
        "\r\n",
        "    self.warmup_steps = warmup_steps\r\n",
        "\r\n",
        "  def __call__(self, step):\r\n",
        "    arg1 = tf.math.rsqrt(step)\r\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\r\n",
        "\r\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\r\n",
        "\r\n",
        "learning_rate = CustomSchedule(128)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \r\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yudWGb8U06WK"
      },
      "source": [
        "model = Transformer()\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \r\n",
        "                                                      reduction='none')\r\n",
        "model.compile(optimizer = optimizer, \r\n",
        "              loss = loss_object)\r\n",
        "history = model.fit(dataset, epochs = 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jlGFNN_TKic",
        "outputId": "2874d1c0-c76b-4eae-f493-94c747797942"
      },
      "source": [
        "for b in dataset:\r\n",
        "    source = b['source']\r\n",
        "    target = b['target']\r\n",
        "    source = source[0]\r\n",
        "    target = target[0][:-1]\r\n",
        "    source = np.expand_dims(source, axis=0)\r\n",
        "    target = np.expand_dims(target, axis=0)\r\n",
        "    out = model([source, target])\r\n",
        "    out = tf.argmax(out, axis=-1)\r\n",
        "    print([idx2word_summary[i] for i in out.numpy()[0]])\r\n",
        "    print([idx2word_summary[i] for i in target[0]])\r\n",
        "    break"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple', 'has', 'unveiled', 'a', 'new', 'low', 'cost', 'macintosh', 'computer', 'for', 'the', 'masses', 'billed', 'as', 'the', 'mac', 'mini', 'the', '499', 'macintosh', 'sold', 'for', '£339', 'in', 'the', 'uk', 'was', 'described', 'by', 'jobs', 'as', 'the', 'most', 'important', 'mac', 'made', 'by', 'apple', 'the', 'smaller', 'ipod', 'will', 'hold', 'about', '120', 'songs', 'said', 'mr', 'jobs', 'mr', 'jobs', 'also', 'unveiled', 'the', 'ipod', '<unk>', 'a', 'new', 'music', 'player', 'using', 'cheaper', 'flash', 'memory', 'rather', 'than', 'hard', 'drives', 'which', 'are', 'used', 'in', 'more', 'expensive', 'ipods', 'in', 'january', 'apple', 'sued', 'a', 'website', 'after', 'it', 'published', 'what', 'it', 'said', 'were', 'specifications', 'for', 'the', 'new', 'computer', 'ian', 'harris', 'deputy', 'editor', 'of', 'uk', 'magazine', 'mac', 'format', 'said', 'the', 'machine', 'would', 'appeal', 'to', 'pc', 'owning', 'consumers', 'who', 'had', 'purchased', 'an', 'ipod', 'the', 'new', 'computer', 'has', 'been', 'the', 'subject', 'of', 'speculation', 'for', 'several', 'weeks', 'and', 'while', 'few', 'people', 'will', 'be', 'surprised', 'by', 'the', 'announcement', 'many', 'analysts', 'had', 'already', 'said', 'it', 'was', 'a', 'sensible', 'move', 'mr', 'jobs', 'told', 'the', 'delegates', 'at', '<unk>', 'that', 'ipod', 'already', 'had', 'a', '65', 'market', 'share', 'of', 'all', 'digital', 'music', 'players', 'in', 'an', 'attempt', 'to', 'win', 'over', 'windows', 'pc', 'customers', 'mr', 'jobs', 'said', 'it', 'would', 'appeal', 'to', 'people', 'thinking', 'of', 'changing', 'operating', 'systems', '<end>', '<unk>', '<unk>', 'three', 'three', 'bringing', 'three', 'bond', 'over', 'three', 'three', 'electronic', 'three', 'three', 'over', 'three', 'three', 'three', 'bringing', 'three', 'three', 'bond', 'three', 'used', 'three', 'three', 'finalised', 'three', 'three', '<unk>', 'term', 'three', 'thinking', 'future', 'three', 'three', 'three', 'three', 'over', 'three', 'three', 'term', 'would', 'three', 'three', 'term', 'three', 'thinking', 'apple', 'three', 'three', 'term', 'three', 'three', 'three', 'three', 'over', 'three', 'three', 'at', 'three', 'few', 'term', 'operating', 'term', 'music', 'an', 'term', 'bond', 'three', 'looking', 'had', 'bringing', 'had', 'term', 'term', 'flash', '<end>', 'as', 'three', 'term', 'looking', 'term', 'three', 'three', 'term', 'looking', 'music', 'three', 'three', 'bringing', 'an', 'term', 'an', 'limited', 'three', 'term', '<end>', 'term', 'explained', 'few', 'mr', 'to', 'future', 'three', 'term', 'three', 'ipods', 'sensible', 'three', '<end>', 'three', 'an', 'three', 'three', 'three', 'owning', 'term', 'consumers', 'three', 'customers', 'term', 'three', 'an', 'term', 'bringing', 'music', 'term', 'term', '65', 'three', 'in', 'three', 'term', 'who', 'three', 'would', 'to', 'looking', 'an', 'over', 'three', 'in', 'three', 'term', 'had', 'high', 'would', 'what', 'three', 'was', 'subject', 'three', 'term', 'term', 'term', 'also', 'sued', 'three', 'three', 'surprised', 'three', 'term', 'the', 'term', 'three', 'original', 'few', 'three', 'flash', 'term', 'to', 'term', 'surprised', 'term', 'already', 'three', 'apple', 'had', 'expensive', 'term', 'three', 'bringing', 'at', 'three', 'had', 'three', 'three', 'term', 'three', 'three', 'the', 'three', 'three', 'three', 'three', 'an', 'term', 'term', 'three', 'three', 'over', 'three', 'three', 'three', 'term', 'term', 'high', 'three', 'high', 'three', 'bringing', 'three', '120', 'term', 'genuine', 'three', 'three', 'three', 'three', 'term', 'move', 'term', 'three', 'three', 'at', 'three', '<unk>', 'said', 'high', 'an', 'and', 'music', 'three', 'three', 'term', 'term', 'three', 'three', '<end>', 'three', 'the', 'three', 'three', 'term', 'term', 'three', 'term', 'three', 'few', 'term', 'term', 'three', 'three', 'three', 'already', 'an', 'three', 'three', 'term', 'three', 'three', 'three', 'three', 'the', 'term', 'three', 'three', 'three', 'apple', 'three', 'term', 'term', 'high', 'already', 'would', 'three', 'three', 'three', 'three', 'three', 'three', 'three', 'three', 'three', 'the', 'three', 'bringing', 'three', 'three', 'three', 'bringing', 'three', 'three', 'market', 'term', 'bond', 'three', 'term', 'term', 'term', 'three', 'three', 'three', 'three', 'the', 'jobs', 'three', 'three', 'three', 'looking', 'three', 'three', 'many', 'term', 'high', 'three', 'term', 'apple', 'three', 'three', 'to', 'used', 'term', 'the', 'term', 'looking', 'future', 'three', 'three', 'term', 'three', 'three', 'three', 'three', 'win', 'three', 'three', 'three', 'would', 'three', 'three', 'three', 'three', 'three', 'three', '<unk>', 'three', 'an', 'three', 'music', 'aimed', 'term', 'many', 'the', 'three', 'three', 'three', 'three', 'three', 'move', 'three', 'three', 'term', 'three', 'three', 'three', 'three', 'term', 'term', 'three', 'three', 'leading', 'three', 'three', 'term', 'looking', 'three', 'three', 'three', 'music', 'three', 'high', 'three', 'three', 'three', 'three', 'three', 'analysts', 'an', 'term', 'several', 'three', 'three', 'an', 'three', 'three', 'three', 'three', 'three', 'three', 'three', 'three', 'term', 'an', 'three', 'future', 'three', 'three', 'three', 'made', 'three', '<unk>', 'as', '<unk>', 'term', 'three', 'three', 'which', 'three', 'looking', 'three', 'three', 'three', 'three', 'three', 'three', 'term', 'music', 'three', 'term', 'future', 'three', 'three', 'three', 'three', 'an', 'three', '<unk>', 'three', 'three', 'term', 'term', 'term', 'three', 'three', 'three', 'was', 'three', 'three', 'time', 'three', 'computer', 'mr', 'looking', 'three', 'three', 'magazine', 'looking', 'sued', 'three', 'three', 'three', 'term', 'three', 'term', 'three', 'three', 'term', 'who', 'three', 'three', 'three', 'term', 'three', 'three', 'three', 'term', 'an', 'future', 'three', 'looking', 'three', 'an', 'apple', 'three', 'three', 'three', 'three', 'to', 'three', 'three', 'an', 'three', 'three', 'three', 'three', 'changing', 'three', 'term', 'three', 'three', 'term', 'genuine', 'bringing', 'three', 'three', 'three', 'looking', 'term']\n",
            "['<start>', 'apple', 'has', 'unveiled', 'a', 'new', 'low', 'cost', 'macintosh', 'computer', 'for', 'the', 'masses', 'billed', 'as', 'the', 'mac', 'mini', 'the', '499', 'macintosh', 'sold', 'for', '£339', 'in', 'the', 'uk', 'was', 'described', 'by', 'jobs', 'as', 'the', 'most', 'important', 'mac', 'made', 'by', 'apple', 'the', 'smaller', 'ipod', 'will', 'hold', 'about', '120', 'songs', 'said', 'mr', 'jobs', 'mr', 'jobs', 'also', 'unveiled', 'the', 'ipod', '<unk>', 'a', 'new', 'music', 'player', 'using', 'cheaper', 'flash', 'memory', 'rather', 'than', 'hard', 'drives', 'which', 'are', 'used', 'in', 'more', 'expensive', 'ipods', 'in', 'january', 'apple', 'sued', 'a', 'website', 'after', 'it', 'published', 'what', 'it', 'said', 'were', 'specifications', 'for', 'the', 'new', 'computer', 'ian', 'harris', 'deputy', 'editor', 'of', 'uk', 'magazine', 'mac', 'format', 'said', 'the', 'machine', 'would', 'appeal', 'to', 'pc', 'owning', 'consumers', 'who', 'had', 'purchased', 'an', 'ipod', 'the', 'new', 'computer', 'has', 'been', 'the', 'subject', 'of', 'speculation', 'for', 'several', 'weeks', 'and', 'while', 'few', 'people', 'will', 'be', 'surprised', 'by', 'the', 'announcement', 'many', 'analysts', 'had', 'already', 'said', 'it', 'was', 'a', 'sensible', 'move', 'mr', 'jobs', 'told', 'the', 'delegates', 'at', '<unk>', 'that', 'ipod', 'already', 'had', 'a', '65', 'market', 'share', 'of', 'all', 'digital', 'music', 'players', 'in', 'an', 'attempt', 'to', 'win', 'over', 'windows', 'pc', 'customers', 'mr', 'jobs', 'said', 'it', 'would', 'appeal', 'to', 'people', 'thinking', 'of', 'changing', 'operating', 'systems', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz_vmF2uzBIo"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "with open('input_tokenizer.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(text_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "with open('output_tokenizer.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(summary_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyZwehFVzqLv"
      },
      "source": [
        "model.save_weights('summary.h5')"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}