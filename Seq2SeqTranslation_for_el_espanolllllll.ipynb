{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqTranslation for el espanolllllll",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNVZwo5TZxXdG2adYOm93k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/Seq2SeqTranslation_for_el_espanolllllll.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yf9_GcB2I4b"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "\r\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\r\n",
        "if not os.path.exists('./datasets'):\r\n",
        "  os.mkdir('./datasets')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBVWZsxI46j"
      },
      "source": [
        "The book, Deep Learning with tensorflow 2 and keras by PACKT, did not have an up to date version. This is my own way to extract the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWIPZbF4CPTM",
        "outputId": "380d4765-e55c-42dd-e9a6-8d15c8cec20e"
      },
      "source": [
        "!wget  -P ./datasets https://www.manythings.org/anki/spa-eng.zip\r\n",
        "!unzip ./datasets/spa-eng.zip -d ./datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-10 20:08:02--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3031::6815:37de, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4934182 (4.7M) [application/zip]\n",
            "Saving to: ‘./datasets/spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.71M  2.72MB/s    in 1.7s    \n",
            "\n",
            "2021-02-10 20:08:04 (2.72 MB/s) - ‘./datasets/spa-eng.zip’ saved [4934182/4934182]\n",
            "\n",
            "Archive:  ./datasets/spa-eng.zip\n",
            "  inflating: ./datasets/_about.txt   \n",
            "  inflating: ./datasets/spa.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uS7DwDb_XdM"
      },
      "source": [
        "def preprocess_sentence(sent):\r\n",
        "  sent = ''.join([c for c in unicodedata.normalize('NFD', sent)])\r\n",
        "  sent = re.sub(r'([!.?])', r' \\1', sent)\r\n",
        "  sent = re.sub(r'[^a-zA-Z!.?]+', r' ', sent)\r\n",
        "  sent = re.sub(r'\\s+', r' ', sent)\r\n",
        "  sent = sent.lower()\r\n",
        "  return sent"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-QXLZ1-AOdb"
      },
      "source": [
        "def download_and_read(num_pairs=50000):\r\n",
        "    en_sents, fr_sents_in, fr_sents_out = [], [], []\r\n",
        "    local_file = os.path.join(\"datasets\", \"spa.txt\")\r\n",
        "    with open(local_file, \"r\") as fin:\r\n",
        "        for i, line in enumerate(fin):\r\n",
        "            en_sent, fr_sent = line.strip().split('CC-BY')[0].strip().split('\\t')\r\n",
        "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\r\n",
        "            fr_sent = preprocess_sentence(fr_sent)\r\n",
        "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\r\n",
        "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\r\n",
        "            en_sents.append(en_sent)\r\n",
        "            fr_sents_in.append(fr_sent_in)\r\n",
        "            fr_sents_out.append(fr_sent_out)\r\n",
        "            if i >= num_pairs-1:\r\n",
        "              break\r\n",
        "    return en_sents, fr_sents_in, fr_sents_out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIR2c4_VEm47",
        "outputId": "a7447944-82b0-48bc-ce2e-9d5f3753f290"
      },
      "source": [
        "sents_en, sents_fr_in, sents_fr_out = download_and_read()\r\n",
        "\r\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters='',lower=False)\r\n",
        "tokenizer_en.fit_on_texts(sents_en)\r\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en)\r\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_en, padding='post')\r\n",
        "\r\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters=\"\", lower=False)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_in)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_out)\r\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\r\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\r\n",
        "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\r\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\r\n",
        "\r\n",
        "vocab_size_en = len(tokenizer_en.word_index)\r\n",
        "vocab_size_fr = len(tokenizer_fr.word_index)\r\n",
        "\r\n",
        "word2idx_en = tokenizer_en.word_index\r\n",
        "idx2word_en = {v:k for k,v in word2idx_en.items()}\r\n",
        "\r\n",
        "word2idx_fr = tokenizer_fr.word_index\r\n",
        "idx2word_fr = {v:k for k,v in word2idx_fr.items()}\r\n",
        "\r\n",
        "print('english vocabulary: ', str (vocab_size_en))\r\n",
        "print('spanish vocabulary: ', str (vocab_size_fr))\r\n",
        "\r\n",
        "maxlen_en = data_en.shape[1]\r\n",
        "maxlen_fr = data_fr_out.shape[1]\r\n",
        "print('The maximum english length is: {:d} '.format(maxlen_en) +   \r\n",
        "  'and the maximum spanish length is: {:d}'.format(maxlen_fr))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english vocabulary:  6639\n",
            "spanish vocabulary:  12364\n",
            "The maximum english length is: 10 and the maximum spanish length is: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aATMp3fvQTWA"
      },
      "source": [
        "batch_size = 64\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (data_en, data_fr_in, data_fr_out))\r\n",
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = 5000 \r\n",
        "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\r\n",
        "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcjmnnRLU8Ve"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps, \r\n",
        "            encoder_dim, **kwargs):\r\n",
        "        super(Encoder, self).__init__(**kwargs)\r\n",
        "        self.encoder_dim = encoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            encoder_dim, return_sequences=False, return_state=True)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, initial_state=state)\r\n",
        "        return x, state\r\n",
        "\r\n",
        "    def init_state(self, batch_size):\r\n",
        "        return tf.zeros((batch_size, self.encoder_dim))\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps,\r\n",
        "            decoder_dim, **kwargs):\r\n",
        "        super(Decoder, self).__init__(**kwargs)\r\n",
        "        self.decoder_dim = decoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            decoder_dim, return_sequences=True, return_state=True)\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, state)\r\n",
        "        x = self.dense(x)\r\n",
        "        return x, state\r\n",
        "\r\n",
        "\r\n",
        "embedding_dim = 128\r\n",
        "encoder_dim, decoder_dim = 512, 512"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dBWPrSF9QLK"
      },
      "source": [
        "def build_graph():\r\n",
        "  encoder = Encoder(vocab_size_en+1,embedding_dim, maxlen_en, encoder_dim)\r\n",
        "  decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)\r\n",
        "  return encoder, decoder"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BxikKzbJKZE"
      },
      "source": [
        "  def loss_fn(ytrue, ypred):\r\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\r\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\r\n",
        "    loss = scce(ytrue, ypred, sample_weight=mask)\r\n",
        "    return loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvVLflXFJLEq"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(encoder, decoder, encoder_in, decoder_in, decoder_out, encoder_state):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "    decoder_state = encoder_state\r\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "    loss = loss_fn(decoder_out, decoder_pred)\r\n",
        "    \r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "  gradients = tape.gradient(loss, variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "  return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ijqXw6ZEme"
      },
      "source": [
        "def predict(encoder, decoder, sents_en, data_en,\r\n",
        "            sents_fr_out, word2idx_fr, idx2word_fr):\r\n",
        "  random_id = np.random.choice(len(sents_en))\r\n",
        "  print('input   : ', ' '.join(sents_en[random_id]))\r\n",
        "  print('label   : ', ' '.join(sents_fr_out[random_id]))\r\n",
        "  encoder_in = tf.expand_dims(data_en[random_id],axis=0)\r\n",
        "  decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\r\n",
        "  encoder_state = encoder.init_state(1)\r\n",
        "  encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "  decoder_state = encoder_state\r\n",
        "  decoder_in = tf.expand_dims(tf.constant([word2idx_fr['BOS']]), axis=0)\r\n",
        "  pred_sent_fr = []\r\n",
        "  while True:\r\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1)\r\n",
        "    pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\r\n",
        "    pred_sent_fr.append(pred_word)\r\n",
        "    if pred_word == 'EOS':\r\n",
        "      break\r\n",
        "    decoder_in = decoder_pred\r\n",
        "  \r\n",
        "  print('predicted   : ', ' '.join(pred_sent_fr[0:-1]))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3JHx7ZBdFSH"
      },
      "source": [
        "def evaluate_bleu_score(encoder, decoder, test_dataset, \r\n",
        "        word2idx_fr, idx2word_fr):\r\n",
        "\r\n",
        "    bleu_scores = []\r\n",
        "    smooth_fn = SmoothingFunction()\r\n",
        "    for encoder_in, decoder_in, decoder_out in test_dataset:\r\n",
        "        encoder_state = encoder.init_state(batch_size)\r\n",
        "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "        decoder_state = encoder_state\r\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "\r\n",
        "        # compute argmax\r\n",
        "        decoder_out = decoder_out.numpy()\r\n",
        "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\r\n",
        "\r\n",
        "        for i in range(decoder_out.shape[0]):\r\n",
        "            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\r\n",
        "            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\r\n",
        "            # remove trailing EOS\r\n",
        "            ref_sent = ref_sent[0:-1]\r\n",
        "            hyp_sent = hyp_sent[0:-1]\r\n",
        "            bleu_score = sentence_bleu([ref_sent], hyp_sent, \r\n",
        "                smoothing_function=smooth_fn.method1)\r\n",
        "            bleu_scores.append(bleu_score)\r\n",
        "\r\n",
        "    return np.mean(np.array(bleu_scores))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kque7khna1Ic"
      },
      "source": [
        "This will take up to an hour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsY1W3mzehRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb68108e-c2cb-4938-8f31-e71db018700d"
      },
      "source": [
        "encoder, decoder = build_graph()\r\n",
        "if not os.path.exists('./checkpoints'):\r\n",
        "  os.mkdir('./checkpoints')\r\n",
        "num_epochs = 150\r\n",
        "for e in range(1,num_epochs+1):\r\n",
        "  encoder_state = encoder.init_state(batch_size)\r\n",
        "  for batch, data in enumerate(train_dataset):\r\n",
        "    encoder_in, decoder_in, decoder_out = data \r\n",
        "    loss = train_step(encoder, decoder, encoder_in, decoder_in, decoder_out, encoder_state)\r\n",
        "  eval_score = evaluate_bleu_score(encoder, decoder,test_dataset,word2idx_fr, idx2word_fr)\r\n",
        "  print('Epoch: {}/{}, Loss: {:.4f}, Eval Score: {:.3e}'.format(e,num_epochs, loss.numpy(),eval_score))\r\n",
        "  if e % 10 == 0:\r\n",
        "    print('PREDICTING TEXT \\n')\r\n",
        "    predict(encoder, decoder, sents_en, data_en, sents_fr_out,word2idx_fr, idx2word_fr)\r\n",
        "    print('\\n')\r\n",
        "  if e % 50 == 0:\r\n",
        "    print('SAVING CHECKPOINT {} ...'.format(str (e // 50)))\r\n",
        "    encoder.save_weights('encoder_ckpt_{}.h5'.format(str (e // 50)))\r\n",
        "    decoder.save_weights('decoder_ckpt_{}.h5'.format(str (e // 50)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/150, Loss: 1.4903, Eval Score: 1.302e-02\n",
            "Epoch: 2/150, Loss: 1.0732, Eval Score: 1.825e-02\n",
            "Epoch: 3/150, Loss: 0.8087, Eval Score: 2.368e-02\n",
            "Epoch: 4/150, Loss: 0.5923, Eval Score: 3.193e-02\n",
            "Epoch: 5/150, Loss: 0.5069, Eval Score: 4.019e-02\n",
            "Epoch: 6/150, Loss: 0.3913, Eval Score: 4.923e-02\n",
            "Epoch: 7/150, Loss: 0.3314, Eval Score: 5.703e-02\n",
            "Epoch: 8/150, Loss: 0.2449, Eval Score: 6.819e-02\n",
            "Epoch: 9/150, Loss: 0.1952, Eval Score: 7.473e-02\n",
            "Epoch: 10/150, Loss: 0.2046, Eval Score: 8.268e-02\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  share it with me .\n",
            "label   :  compa rtelo conmigo . EOS\n",
            "predicted   :  compartilo conmigo .\n",
            "\n",
            "\n",
            "Epoch: 11/150, Loss: 0.1379, Eval Score: 8.670e-02\n",
            "Epoch: 12/150, Loss: 0.1542, Eval Score: 9.268e-02\n",
            "Epoch: 13/150, Loss: 0.1332, Eval Score: 9.655e-02\n",
            "Epoch: 14/150, Loss: 0.1161, Eval Score: 9.933e-02\n",
            "Epoch: 15/150, Loss: 0.1138, Eval Score: 1.028e-01\n",
            "Epoch: 16/150, Loss: 0.1220, Eval Score: 1.041e-01\n",
            "Epoch: 17/150, Loss: 0.0910, Eval Score: 1.066e-01\n",
            "Epoch: 18/150, Loss: 0.0875, Eval Score: 1.095e-01\n",
            "Epoch: 19/150, Loss: 0.1004, Eval Score: 1.103e-01\n",
            "Epoch: 20/150, Loss: 0.0896, Eval Score: 1.111e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  i must run .\n",
            "label   :  tengo que correr . EOS\n",
            "predicted   :  tengo que correr .\n",
            "\n",
            "\n",
            "Epoch: 21/150, Loss: 0.0823, Eval Score: 1.115e-01\n",
            "Epoch: 22/150, Loss: 0.0752, Eval Score: 1.120e-01\n",
            "Epoch: 23/150, Loss: 0.0704, Eval Score: 1.137e-01\n",
            "Epoch: 24/150, Loss: 0.0914, Eval Score: 1.152e-01\n",
            "Epoch: 25/150, Loss: 0.0987, Eval Score: 1.157e-01\n",
            "Epoch: 26/150, Loss: 0.0553, Eval Score: 1.149e-01\n",
            "Epoch: 27/150, Loss: 0.0686, Eval Score: 1.164e-01\n",
            "Epoch: 28/150, Loss: 0.0718, Eval Score: 1.155e-01\n",
            "Epoch: 29/150, Loss: 0.0813, Eval Score: 1.175e-01\n",
            "Epoch: 30/150, Loss: 0.0690, Eval Score: 1.183e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  i have a stomachache .\n",
            "label   :  me duele la panza . EOS\n",
            "predicted   :  me duele el esto mago .\n",
            "\n",
            "\n",
            "Epoch: 31/150, Loss: 0.0835, Eval Score: 1.171e-01\n",
            "Epoch: 32/150, Loss: 0.0778, Eval Score: 1.185e-01\n",
            "Epoch: 33/150, Loss: 0.0575, Eval Score: 1.174e-01\n",
            "Epoch: 34/150, Loss: 0.0835, Eval Score: 1.175e-01\n",
            "Epoch: 35/150, Loss: 0.0594, Eval Score: 1.169e-01\n",
            "Epoch: 36/150, Loss: 0.0647, Eval Score: 1.183e-01\n",
            "Epoch: 37/150, Loss: 0.0719, Eval Score: 1.190e-01\n",
            "Epoch: 38/150, Loss: 0.0526, Eval Score: 1.180e-01\n",
            "Epoch: 39/150, Loss: 0.0653, Eval Score: 1.190e-01\n",
            "Epoch: 40/150, Loss: 0.0793, Eval Score: 1.198e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  the law was changed .\n",
            "label   :  la ley fue alterada . EOS\n",
            "predicted   :  la ley fue alterada .\n",
            "\n",
            "\n",
            "Epoch: 41/150, Loss: 0.0732, Eval Score: 1.175e-01\n",
            "Epoch: 42/150, Loss: 0.0739, Eval Score: 1.197e-01\n",
            "Epoch: 43/150, Loss: 0.0910, Eval Score: 1.177e-01\n",
            "Epoch: 44/150, Loss: 0.1122, Eval Score: 1.213e-01\n",
            "Epoch: 45/150, Loss: 0.0547, Eval Score: 1.194e-01\n",
            "Epoch: 46/150, Loss: 0.1061, Eval Score: 1.184e-01\n",
            "Epoch: 47/150, Loss: 0.0551, Eval Score: 1.199e-01\n",
            "Epoch: 48/150, Loss: 0.0888, Eval Score: 1.184e-01\n",
            "Epoch: 49/150, Loss: 0.0639, Eval Score: 1.187e-01\n",
            "Epoch: 50/150, Loss: 0.0739, Eval Score: 1.203e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  is tom a suspect ?\n",
            "label   :  tom es un sospechoso ? EOS\n",
            "predicted   :  tom es un sospechoso ?\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 1 ...\n",
            "Epoch: 51/150, Loss: 0.0715, Eval Score: 1.202e-01\n",
            "Epoch: 52/150, Loss: 0.0634, Eval Score: 1.187e-01\n",
            "Epoch: 53/150, Loss: 0.0531, Eval Score: 1.196e-01\n",
            "Epoch: 54/150, Loss: 0.0794, Eval Score: 1.213e-01\n",
            "Epoch: 55/150, Loss: 0.0822, Eval Score: 1.199e-01\n",
            "Epoch: 56/150, Loss: 0.1028, Eval Score: 1.222e-01\n",
            "Epoch: 57/150, Loss: 0.0752, Eval Score: 1.197e-01\n",
            "Epoch: 58/150, Loss: 0.0594, Eval Score: 1.194e-01\n",
            "Epoch: 59/150, Loss: 0.0807, Eval Score: 1.202e-01\n",
            "Epoch: 60/150, Loss: 0.0705, Eval Score: 1.210e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  i m going to wait .\n",
            "label   :  voy a esperar . EOS\n",
            "predicted   :  voy a esperar .\n",
            "\n",
            "\n",
            "Epoch: 61/150, Loss: 0.0886, Eval Score: 1.222e-01\n",
            "Epoch: 62/150, Loss: 0.0681, Eval Score: 1.210e-01\n",
            "Epoch: 63/150, Loss: 0.0847, Eval Score: 1.198e-01\n",
            "Epoch: 64/150, Loss: 0.0852, Eval Score: 1.199e-01\n",
            "Epoch: 65/150, Loss: 0.0642, Eval Score: 1.200e-01\n",
            "Epoch: 66/150, Loss: 0.0655, Eval Score: 1.204e-01\n",
            "Epoch: 67/150, Loss: 0.0900, Eval Score: 1.198e-01\n",
            "Epoch: 68/150, Loss: 0.0766, Eval Score: 1.198e-01\n",
            "Epoch: 69/150, Loss: 0.0851, Eval Score: 1.206e-01\n",
            "Epoch: 70/150, Loss: 0.0775, Eval Score: 1.211e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  tom doesn t smoke .\n",
            "label   :  tom no fuma . EOS\n",
            "predicted   :  tom no fuma .\n",
            "\n",
            "\n",
            "Epoch: 71/150, Loss: 0.0521, Eval Score: 1.201e-01\n",
            "Epoch: 72/150, Loss: 0.0292, Eval Score: 1.209e-01\n",
            "Epoch: 73/150, Loss: 0.0642, Eval Score: 1.195e-01\n",
            "Epoch: 74/150, Loss: 0.0375, Eval Score: 1.221e-01\n",
            "Epoch: 75/150, Loss: 0.0619, Eval Score: 1.220e-01\n",
            "Epoch: 76/150, Loss: 0.0757, Eval Score: 1.206e-01\n",
            "Epoch: 77/150, Loss: 0.0651, Eval Score: 1.195e-01\n",
            "Epoch: 78/150, Loss: 0.0567, Eval Score: 1.204e-01\n",
            "Epoch: 79/150, Loss: 0.0525, Eval Score: 1.221e-01\n",
            "Epoch: 80/150, Loss: 0.0657, Eval Score: 1.212e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  tom is losing mary .\n",
            "label   :  tom esta perdiendo a mary . EOS\n",
            "predicted   :  tom esta perdiendo a mary .\n",
            "\n",
            "\n",
            "Epoch: 81/150, Loss: 0.0573, Eval Score: 1.200e-01\n",
            "Epoch: 82/150, Loss: 0.0639, Eval Score: 1.207e-01\n",
            "Epoch: 83/150, Loss: 0.0581, Eval Score: 1.215e-01\n",
            "Epoch: 84/150, Loss: 0.0822, Eval Score: 1.211e-01\n",
            "Epoch: 85/150, Loss: 0.0869, Eval Score: 1.210e-01\n",
            "Epoch: 86/150, Loss: 0.0686, Eval Score: 1.202e-01\n",
            "Epoch: 87/150, Loss: 0.0779, Eval Score: 1.221e-01\n",
            "Epoch: 88/150, Loss: 0.0675, Eval Score: 1.198e-01\n",
            "Epoch: 89/150, Loss: 0.0866, Eval Score: 1.205e-01\n",
            "Epoch: 90/150, Loss: 0.0548, Eval Score: 1.218e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  tom came too late .\n",
            "label   :  tom vino muy tarde . EOS\n",
            "predicted   :  tom vino muy tarde .\n",
            "\n",
            "\n",
            "Epoch: 91/150, Loss: 0.0757, Eval Score: 1.210e-01\n",
            "Epoch: 92/150, Loss: 0.0607, Eval Score: 1.204e-01\n",
            "Epoch: 93/150, Loss: 0.0362, Eval Score: 1.208e-01\n",
            "Epoch: 94/150, Loss: 0.0918, Eval Score: 1.215e-01\n",
            "Epoch: 95/150, Loss: 0.0491, Eval Score: 1.216e-01\n",
            "Epoch: 96/150, Loss: 0.0630, Eval Score: 1.219e-01\n",
            "Epoch: 97/150, Loss: 0.0605, Eval Score: 1.218e-01\n",
            "Epoch: 98/150, Loss: 0.0594, Eval Score: 1.220e-01\n",
            "Epoch: 99/150, Loss: 0.0729, Eval Score: 1.214e-01\n",
            "Epoch: 100/150, Loss: 0.0465, Eval Score: 1.212e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  they ve left .\n",
            "label   :  se han ido . EOS\n",
            "predicted   :  se han ido .\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 2 ...\n",
            "Epoch: 101/150, Loss: 0.0491, Eval Score: 1.218e-01\n",
            "Epoch: 102/150, Loss: 0.0546, Eval Score: 1.212e-01\n",
            "Epoch: 103/150, Loss: 0.0537, Eval Score: 1.211e-01\n",
            "Epoch: 104/150, Loss: 0.0659, Eval Score: 1.204e-01\n",
            "Epoch: 105/150, Loss: 0.0554, Eval Score: 1.219e-01\n",
            "Epoch: 106/150, Loss: 0.0612, Eval Score: 1.226e-01\n",
            "Epoch: 107/150, Loss: 0.0679, Eval Score: 1.213e-01\n",
            "Epoch: 108/150, Loss: 0.0559, Eval Score: 1.216e-01\n",
            "Epoch: 109/150, Loss: 0.0696, Eval Score: 1.210e-01\n",
            "Epoch: 110/150, Loss: 0.0762, Eval Score: 1.195e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  tom did worse this year .\n",
            "label   :  tom lo hizo peor este an o . EOS\n",
            "predicted   :  tom lo hizo peor este an o .\n",
            "\n",
            "\n",
            "Epoch: 111/150, Loss: 0.0652, Eval Score: 1.220e-01\n",
            "Epoch: 112/150, Loss: 0.0857, Eval Score: 1.217e-01\n",
            "Epoch: 113/150, Loss: 0.0709, Eval Score: 1.204e-01\n",
            "Epoch: 114/150, Loss: 0.0865, Eval Score: 1.216e-01\n",
            "Epoch: 115/150, Loss: 0.0743, Eval Score: 1.194e-01\n",
            "Epoch: 116/150, Loss: 0.0688, Eval Score: 1.215e-01\n",
            "Epoch: 117/150, Loss: 0.0508, Eval Score: 1.209e-01\n",
            "Epoch: 118/150, Loss: 0.0613, Eval Score: 1.221e-01\n",
            "Epoch: 119/150, Loss: 0.0651, Eval Score: 1.212e-01\n",
            "Epoch: 120/150, Loss: 0.0657, Eval Score: 1.229e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  i don t like that word .\n",
            "label   :  no me gusta esa palabra . EOS\n",
            "predicted   :  no me gusta esa palabra .\n",
            "\n",
            "\n",
            "Epoch: 121/150, Loss: 0.0612, Eval Score: 1.222e-01\n",
            "Epoch: 122/150, Loss: 0.0562, Eval Score: 1.220e-01\n",
            "Epoch: 123/150, Loss: 0.0598, Eval Score: 1.230e-01\n",
            "Epoch: 124/150, Loss: 0.0764, Eval Score: 1.226e-01\n",
            "Epoch: 125/150, Loss: 0.0573, Eval Score: 1.224e-01\n",
            "Epoch: 126/150, Loss: 0.0712, Eval Score: 1.223e-01\n",
            "Epoch: 127/150, Loss: 0.0420, Eval Score: 1.217e-01\n",
            "Epoch: 128/150, Loss: 0.0611, Eval Score: 1.216e-01\n",
            "Epoch: 129/150, Loss: 0.0429, Eval Score: 1.213e-01\n",
            "Epoch: 130/150, Loss: 0.0438, Eval Score: 1.217e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  how did tom look ?\n",
            "label   :  co mo se vei a tom ? EOS\n",
            "predicted   :  co mo se vei a tom ?\n",
            "\n",
            "\n",
            "Epoch: 131/150, Loss: 0.0443, Eval Score: 1.220e-01\n",
            "Epoch: 132/150, Loss: 0.0603, Eval Score: 1.206e-01\n",
            "Epoch: 133/150, Loss: 0.0694, Eval Score: 1.211e-01\n",
            "Epoch: 134/150, Loss: 0.0697, Eval Score: 1.204e-01\n",
            "Epoch: 135/150, Loss: 0.0662, Eval Score: 1.230e-01\n",
            "Epoch: 136/150, Loss: 0.0716, Eval Score: 1.208e-01\n",
            "Epoch: 137/150, Loss: 0.0311, Eval Score: 1.212e-01\n",
            "Epoch: 138/150, Loss: 0.0615, Eval Score: 1.225e-01\n",
            "Epoch: 139/150, Loss: 0.0603, Eval Score: 1.207e-01\n",
            "Epoch: 140/150, Loss: 0.0544, Eval Score: 1.186e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  good luck with that tom .\n",
            "label   :  suerte con eso tom . EOS\n",
            "predicted   :  suerte con eso tom .\n",
            "\n",
            "\n",
            "Epoch: 141/150, Loss: 0.0641, Eval Score: 1.208e-01\n",
            "Epoch: 142/150, Loss: 0.0573, Eval Score: 1.202e-01\n",
            "Epoch: 143/150, Loss: 0.0428, Eval Score: 1.209e-01\n",
            "Epoch: 144/150, Loss: 0.0494, Eval Score: 1.218e-01\n",
            "Epoch: 145/150, Loss: 0.1100, Eval Score: 1.225e-01\n",
            "Epoch: 146/150, Loss: 0.0759, Eval Score: 1.221e-01\n",
            "Epoch: 147/150, Loss: 0.0562, Eval Score: 1.211e-01\n",
            "Epoch: 148/150, Loss: 0.0491, Eval Score: 1.204e-01\n",
            "Epoch: 149/150, Loss: 0.0819, Eval Score: 1.211e-01\n",
            "Epoch: 150/150, Loss: 0.0617, Eval Score: 1.197e-01\n",
            "PREDICTING TEXT \n",
            "\n",
            "input   :  you re depressing me .\n",
            "label   :  me esta s bajoneando . EOS\n",
            "predicted   :  me esta s bajoneando .\n",
            "\n",
            "\n",
            "SAVING CHECKPOINT 3 ...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}