{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS Spam Detection",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDaL5h3joEIVoSY7pqKTrg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/SMS_Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_npqHao5HXLB"
      },
      "source": [
        "import gensim.downloader as api\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMUwVT0aJMbP"
      },
      "source": [
        "def download_and_read(url):\r\n",
        "  local_file = url.split('/')[-1]\r\n",
        "  p = tf.keras.utils.get_file(local_file,url,extract=True,cache_dir='.')\r\n",
        "  labels, texts = [], []\r\n",
        "  local_file = os.path.join('datasets','SMSSpamCollection')\r\n",
        "  with open(local_file, 'r') as fin:\r\n",
        "    for line in fin:\r\n",
        "      label, text = line.strip().split('\\t')\r\n",
        "      labels.append(1 if label == 'spam' else 0)\r\n",
        "      texts.append(text)\r\n",
        "  return labels,texts"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waw6HNOyLQJY"
      },
      "source": [
        "DATASET_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\r\n",
        "labels,texts = download_and_read(DATASET_URL)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiFL6qE8MeTi",
        "outputId": "66b8a316-3bea-41ff-a326-acc12678dd37"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\r\n",
        "tokenizer.fit_on_texts(texts)\r\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\r\n",
        "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\r\n",
        "num_records = len(text_sequences)\r\n",
        "max_seqlen = len(text_sequences[0])\r\n",
        "print('{:d} senteces, max len :{:d}'.format(num_records,max_seqlen))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5574 senteces, max len :189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPdTPD6l592t"
      },
      "source": [
        "cat_labels = tf.keras.utils.to_categorical(labels,2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu7p1bJ7QVHh",
        "outputId": "7d01f6c3-afb1-4a94-8eff-64ab88dcbfc3"
      },
      "source": [
        "word2idx = tokenizer.word_index\r\n",
        "idx2word = {v:k for k, v in word2idx.items()}\r\n",
        "word2idx['PAD'] = 0\r\n",
        "idx2word[0] = 'PAD'\r\n",
        "vocab_size = len(word2idx)\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  9010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr9I_XKLRHxp"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((text_sequences,cat_labels))\r\n",
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = num_records // 10\r\n",
        "val_size = num_records // 10\r\n",
        "test_dataset = dataset.take(test_size)\r\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\r\n",
        "train_dataset = dataset.skip(test_size + val_size)\r\n",
        "\r\n",
        "BATCH_SIZE = 128\r\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE,drop_remainder=True)\r\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE,drop_remainder=True)\r\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE,drop_remainder=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PatDe8emTJMG"
      },
      "source": [
        "def build_embedding_matrix(sequences,word2idx,embedding_dim,embedding_file):\r\n",
        "  if os.path.exists(embedding_file):\r\n",
        "    print('Loading cached file ...')\r\n",
        "    E = np.load(embedding_file)\r\n",
        "    \r\n",
        "  \r\n",
        "  else:\r\n",
        "    vocab_size = len(word2idx)\r\n",
        "    E = np.zeros((vocab_size,embedding_dim))\r\n",
        "    word_vectors = api.load(EMBEDDING_MODEL)\r\n",
        "    for word, idx, in word2idx.items():\r\n",
        "      try:\r\n",
        "         E[idx] = word_vectors.word_vec(word)\r\n",
        "      except KeyError:\r\n",
        "        pass\r\n",
        "    np.save(embedding_file,E)\r\n",
        "  return E"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Amn1ggbswU",
        "outputId": "af01ad36-68eb-41dd-a455-8a3af5ebacf5"
      },
      "source": [
        "EMBEDDING_DIM = 300\r\n",
        "DATA_DIR = 'data'\r\n",
        "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR,'E.npy')\r\n",
        "EMBEDDING_MODEL = 'glove-wiki-gigaword-300'\r\n",
        "E = build_embedding_matrix(text_sequences,word2idx,EMBEDDING_DIM,EMBEDDING_NUMPY_FILE)\r\n",
        "print('Embedding matrix: ', E.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached file ...\n",
            "Embedding matrix:  (9010, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qa_UcJefztM"
      },
      "source": [
        "class SpamClassifierModel(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_sz, embed_sz, input_length, num_filters, kernel_sz,\r\n",
        "               output_sz, run_mode, embedding_weights, **kwargs):\r\n",
        "    super(SpamClassifierModel,self).__init__(**kwargs)\r\n",
        "    if run_mode == 'scratch':\r\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_sz,embed_sz,\r\n",
        "                                                 input_length=input_length, \r\n",
        "                                                 trainable=True)\r\n",
        "    else:\r\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_sz, embed_sz,\r\n",
        "                                                 input_length = input_length,\r\n",
        "                                                 weights=[embedding_weights],\r\n",
        "                                                 trainable=False)\r\n",
        "    self.conv = tf.keras.layers.Conv1D(filters=num_filters,kernel_size=kernel_sz,\r\n",
        "                                       activation='relu')\r\n",
        "    self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\r\n",
        "    self.pool = tf.keras.layers.GlobalMaxPool1D()\r\n",
        "    self.dense = tf.keras.layers.Dense(output_sz,activation='softmax')\r\n",
        "  \r\n",
        "  def call(self, x):\r\n",
        "    x = self.embedding(x)\r\n",
        "    x = self.conv(x)\r\n",
        "    x = self.dropout(x)\r\n",
        "    x = self.pool(x)\r\n",
        "    x = self.dense(x)\r\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln2LXGlYmdye"
      },
      "source": [
        "We will make the next part a function, so that we can run the code with a GPU in the same tensorflow graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVia6b54mUaR"
      },
      "source": [
        "\r\n",
        "def get_model():\r\n",
        "  model = SpamClassifierModel(vocab_size,EMBEDDING_DIM,max_seqlen,256,3,2,' ',E)\r\n",
        "  model.build(input_shape=(None,max_seqlen))\r\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\r\n",
        "  return model\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy6ZIrX62LfQ",
        "outputId": "a0fd0744-1653-4942-c22e-a4709195858b"
      },
      "source": [
        "EPOCHS = 3\r\n",
        "\r\n",
        "CLASS_WEIGHTS = {0:1,1:8}\r\n",
        "\r\n",
        "with tf.device('gpu:0'):\r\n",
        "  model = get_model()\r\n",
        "  model.fit(train_dataset,epochs=EPOCHS,validation_data = val_dataset, class_weight=CLASS_WEIGHTS)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "34/34 [==============================] - 16s 443ms/step - loss: 0.7154 - accuracy: 0.7920 - val_loss: 0.1194 - val_accuracy: 0.9648\n",
            "Epoch 2/3\n",
            "34/34 [==============================] - 14s 422ms/step - loss: 0.1629 - accuracy: 0.9611 - val_loss: 0.0477 - val_accuracy: 0.9902\n",
            "Epoch 3/3\n",
            "34/34 [==============================] - 14s 427ms/step - loss: 0.0955 - accuracy: 0.9856 - val_loss: 0.0274 - val_accuracy: 0.9980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShaFUXF23HDv",
        "outputId": "f80ecee0-b778-4ebb-bf26-4a16fa41c0aa"
      },
      "source": [
        "labels, predictions = [], []\r\n",
        "for xtest, ytest in test_dataset:\r\n",
        "  ytest_ = model.predict_on_batch(xtest)\r\n",
        "  ytest = np.argmax(ytest,axis=1)\r\n",
        "  ytest_ = np.argmax(ytest_,axis=1)\r\n",
        "  labels.extend(ytest.tolist())\r\n",
        "  predictions.extend(ytest_.tolist())\r\n",
        "\r\n",
        "print('confusion matrix')\r\n",
        "print(confusion_matrix(labels,predictions))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix\n",
            "[[446   3]\n",
            " [  0  63]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}